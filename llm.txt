Directory structure:
└── xronforce-xronai/
    ├── README.md
    ├── LICENSE
    ├── pyproject.toml
    ├── requirements.txt
    ├── examples/
    │   ├── agent_history_basic.py
    │   ├── agent_tool_usage_benchmark.py
    │   ├── agent_with_mcp_tools.py
    │   ├── agent_with_remote_mcp_sse.py
    │   ├── chat_history_comparison.py
    │   ├── chat_history_display.py
    │   ├── eda.py
    │   ├── hierarchical_structure.py
    │   ├── history_loading.py
    │   ├── history_loading_with_assistant_supervisor.py
    │   ├── multi_argument_function_calling.py
    │   ├── multi_logger.py
    │   ├── multiple_tool_calls.py
    │   ├── output_schema_examples.py
    │   ├── schema_aware_workflow_example.py
    │   ├── workflow_with_custom_id.py
    │   ├── config_mcp_schema/
    │   │   ├── config.yaml
    │   │   └── config_mcp_schema.py
    │   ├── memory_task_management/
    │   │   ├── config.yaml
    │   │   ├── example_memory_management.py
    │   │   └── task_tools.py
    │   ├── supervisor_multi_mcp/
    │   │   ├── README.md
    │   │   ├── add_server.py
    │   │   ├── example_multi_mcp.py
    │   │   └── weather_server.py
    │   └── task_management_with_yaml/
    │       ├── config.yaml
    │       ├── example_task_management.py
    │       └── task_tools.py
    ├── src/
    │   └── xronai/
    │       ├── __init__.py
    │       ├── config/
    │       │   ├── __init__.py
    │       │   ├── agent_factory.py
    │       │   ├── config_validator.py
    │       │   └── yaml_config.py
    │       ├── core/
    │       │   ├── __init__.py
    │       │   ├── agents.py
    │       │   ├── ai.py
    │       │   └── supervisor.py
    │       ├── history/
    │       │   ├── __init__.py
    │       │   └── history_manager.py
    │       └── utils/
    │           ├── __init__.py
    │           └── debugger.py
    ├── studio/
    │   ├── __init__.py
    │   ├── cli.py
    │   ├── server/
    │   │   ├── __init__.py
    │   │   ├── graph_utils.py
    │   │   ├── main.py
    │   │   └── state.py
    │   └── ui/
    │       ├── app.js
    │       ├── index.html
    │       └── style.css
    └── .github/
        └── workflows/
            └── publish-to-pypi.yml


================================================
FILE: README.md
================================================
# xronai
XronAI: The Python SDK for building powerful, agentic AI chatbots.



================================================
FILE: LICENSE
================================================
MIT License

Copyright (c) 2025 Xron Force

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



================================================
FILE: pyproject.toml
================================================
[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "xronai"
version = "0.1.3" # Start with an initial version
authors = [
  { name="Mubashir ul Islam", email="mubashir54000@gmail.com" },
]
description = "XronAI: The Python SDK for building powerful, agentic AI chatbots."
readme = "README.md"
requires-python = ">=3.10"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License", # Assuming you use MIT, change if needed
    "Operating System :: OS Independent",
    "Intended Audience :: Developers",
    "Topic :: Software Development :: Libraries :: Python Modules",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
]
keywords = ["ai", "sdk", "chatbot", "agent", "llm", "orchestration"]
dependencies = [ 
    "openai==1.66.3", 
    "python-dotenv==1.0.1", 
    "pytest==8.3.4", 
    "PyYAML==6.0.2", 
    "mcp[cli]==1.6.0",
]

[project.optional-dependencies]
studio = [
    "fastapi==0.116.1",
    "uvicorn[standard]==0.35.0",
    "websockets==15.0.1",
    "typer[all]==0.17.4",
]

[project.urls]
"Homepage" = "https://github.com/XronForce/xronai"
"Bug Tracker" = "https://github.com/XronForce/xronai/issues"

[project.scripts]
xronai = "studio.cli:app"

# This section is CRUCIAL for the src layout
[tool.setuptools.packages.find]
where = ["src", "."]


================================================
FILE: requirements.txt
================================================
openai==1.66.3
python-dotenv==1.0.1
streamlit>=1.38.0
pytest==8.3.4
PyYAML==6.0.2
mcp[cli]>=1.10.0


================================================
FILE: examples/agent_history_basic.py
================================================
"""
Example: Standalone Agent with working persistent history.jsonl
"""
import os
import sys
from dotenv import load_dotenv

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from src.xronai.core import Agent

load_dotenv()

llm_config = {
    "model": os.getenv('LLM_MODEL'),
    "api_key": os.getenv('LLM_API_KEY'),
    "base_url": os.getenv('LLM_BASE_URL')
}

WORKFLOW_ID = "agent_history_demo_new" # Use a new ID for a clean test
system_message = "You are a friendly assistant who remembers our conversation history."

def main():
    print(f"\n--- Standalone Agent with AUTOMATIC Persistent History ---")
    print(f"Workflow ID: {WORKFLOW_ID}\n")

    # The Agent now handles all history setup internally!
    agent = Agent(
        name="SoloAgent",
        llm_config=llm_config,
        workflow_id=WORKFLOW_ID,
        system_message=system_message,
        keep_history=True
    )

    if len(agent.chat_history) > 1:
        print("[Info] Previous history loaded for SoloAgent.\n")

    print('Type your messages! "show" displays chat history, "exit" to quit.')

    while True:
        msg = input("\nYou: ").strip()
        if msg.lower() == "exit":
            print("Session ended.")
            break
        elif msg.lower() == "show":
            print("=== Agent Chat History ===")
            for m in agent.chat_history:
                print(f"{m['role']}: {m['content']}")
            continue
        try:
            answer = agent.chat(msg)
            print("Agent:", answer)
        except Exception as e:
            print("Error:", e)

    print(f"\nHistory is now saved in xronai_logs/{WORKFLOW_ID}/history.jsonl")

if __name__ == "__main__":
    main()


================================================
FILE: examples/agent_tool_usage_benchmark.py
================================================
import os, sys
from dotenv import load_dotenv
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from src.xronai.core import Agent, Supervisor

# Load environment variables for LLM credentials
load_dotenv()

llm_config = {
    'model': os.getenv('LLM_MODEL'),
    'api_key': os.getenv('LLM_API_KEY'),
    'base_url': os.getenv('LLM_BASE_URL')
}

# === Define Two Tools for a "MathAgent" ===
def add(a: int, b: int) -> int:
    """Add two numbers."""
    return a + b

def multiply(a: int, b: int) -> int:
    """Multiply two numbers."""
    return a * b

math_tools = [
    {
        "tool": add,
        "metadata": {
            "type": "function",
            "function": {
                "name": "add",
                "description": "Add two numbers.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "a": {"type": "integer", "description": "First number"},
                        "b": {"type": "integer", "description": "Second number"}
                    },
                    "required": ["a", "b"]
                }
            }
        }
    },
    {
        "tool": multiply,
        "metadata": {
            "type": "function",
            "function": {
                "name": "multiply",
                "description": "Multiply two numbers.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "a": {"type": "integer", "description": "First number"},
                        "b": {"type": "integer", "description": "Second number"}
                    },
                    "required": ["a", "b"]
                }
            }
        }
    }
]

math_agent = Agent(
    name="MathAgent",
    llm_config=llm_config,
    system_message="You are an agent skilled at performing arithmetic calculations like addition and multiplication. "
                  "When asked to calculate values, use your tools. If a question involves multiple calculations, use all tools as needed."
                  "You should use Agents and tools in parallel if possible.",
    tools=math_tools,
    use_tools=True
)

# === Define Agent 2: TextAgent with two tools: upper and reverse ===
def to_uppercase(text: str) -> str:
    return text.upper()

def reverse_text(text: str) -> str:
    return text[::-1]

text_tools = [
    {
        "tool": to_uppercase,
        "metadata": {
            "type": "function",
            "function": {
                "name": "to_uppercase",
                "description": "Convert text to uppercase.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "text": {"type": "string", "description": "Text to convert"}
                    },
                    "required": ["text"]
                }
            }
        }
    },
    {
        "tool": reverse_text,
        "metadata": {
            "type": "function",
            "function": {
                "name": "reverse_text",
                "description": "Reverse the given text.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "text": {"type": "string", "description": "Text to reverse"}
                    },
                    "required": ["text"]
                }
            }
        }
    }
]

text_agent = Agent(
    name="TextAgent",
    llm_config=llm_config,
    system_message="You are a helpful text processing agent, skilled in text transformation tasks like uppercase conversion and text reversal.",
    tools=text_tools,
    use_tools=True
)

# === Supervisor Setup ===
supervisor = Supervisor(
    name="SuperSupervisor",
    llm_config=llm_config,
    system_message="You are a manager who delegates math and text tasks to the appropriate agents. For user requests involving both math and text, delegate each part to the correct agent so all results are collected in one answer."
)
supervisor.register_agent(math_agent)
supervisor.register_agent(text_agent)

supervisor.display_agent_graph()

if __name__ == "__main__":
    print("\nDemo: Multi-Agent, Multi-Tool in One Turn\n")
    complex_query = (
        "Please do all of the following:"
        "\n1. Add 7 and 3, and multiply 4 and 6."
        "\n2. Convert 'hello world' to uppercase and also reverse 'chatbot'."
    )
    print(f"User: {complex_query}\n")

    response = supervisor.chat(complex_query)
    print(f"\nSuperSupervisor: {response}")
    print("\nYou just witnessed a response where Supervisor delegated to both agents, and MathAgent performed two tool calls in one reply (add & multiply), and TextAgent likewise (uppercase & reverse).")


================================================
FILE: examples/agent_with_mcp_tools.py
================================================
import os, sys
from dotenv import load_dotenv

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from src.xronai.core import Agent

# Load environment variables
load_dotenv()

# LLM Configuration
llm_config = {
    'model': os.getenv('LLM_MODEL'),
    'api_key': os.getenv('LLM_API_KEY'),
    'base_url': os.getenv('LLM_BASE_URL')
}

# Setup MCP server config (local, no auth)
mcp_servers = [{"type": "sse", "url": "http://localhost:8000/sse", "auth_token": None}]

agent = Agent(name="MCPAgent",
              llm_config=llm_config,
              system_message="You are an agent using MCP tools.",
              mcp_servers=mcp_servers)

print("=== Tools discovered from MCP server ===")
for t in agent.tools:
    meta = t["metadata"]["function"]
    print(f"- {meta['name']}: {meta.get('description')}")
print("")

# Find the "add" tool from MCP
add_tool = next((t for t in agent.tools if t["metadata"]["function"]["name"] == "add"), None)
if not add_tool:
    print("No 'add' tool found!")
    exit(1)

print("Calling 'add' tool from agent (add(2, 3)):")
result = add_tool["tool"](a=2, b=3)
print("Result:", result)

# Now test MCP tool update (simulate adding new tool in MCP server before calling)
input("\nAdd another tool in your running MCP server and press enter to update MCP tools...")

agent.update_mcp_tools()
print("\n=== Tools after update ===")
for t in agent.tools:
    meta = t["metadata"]["function"]
    print(f"- {meta['name']}: {meta.get('description')}")



================================================
FILE: examples/agent_with_remote_mcp_sse.py
================================================
import os, sys
from dotenv import load_dotenv

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from src.xronai.core import Agent

# Load environment variables
load_dotenv()

# LLM Configuration
llm_config = {
    'model': os.getenv('LLM_MODEL'),
    'api_key': os.getenv('LLM_API_KEY'),
    'base_url': os.getenv('LLM_BASE_URL')
}

# Configure to use the remote SSE MCP server
mcp_servers = [{
    "type": "sse",
    "url": "https://remote.mcpservers.org/fetch",
    # "auth_token": "YOUR_TOKEN_IF_NEEDED",  # Add if the server requires auth
}]

agent = Agent(name="RemoteMCPAgent",
              llm_config=llm_config,
              mcp_servers=mcp_servers,
              system_message="You are an agent with access to remote tools via MCP.",
              use_tools=True)

print("Available tools from remote MCP server:")
for t in agent.tools:
    print("-", t["metadata"]["function"]["name"])

print("\nType 'exit' to quit.")
while True:
    question = input("\nYou: ")
    if question.strip().lower() == "exit":
        break
    try:
        answer = agent.chat(question)
        print("Agent:", answer)
    except Exception as e:
        print("Error:", e)



================================================
FILE: examples/chat_history_comparison.py
================================================
import os, sys
from dotenv import load_dotenv
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from src.xronai.core import Agent

# Load environment variables
load_dotenv()

# LLM Configuration
llm_config = {
    'model': os.getenv('LLM_MODEL'),
    'api_key': os.getenv('LLM_API_KEY'),
    'base_url': os.getenv('LLM_BASE_URL')
}

# Initialize two agents: one with history and one without
agent_1 = Agent(
    name="Agent1",
    system_message="You are a helpful assistant.",
    llm_config=llm_config,
    keep_history=True  # default behavior
)

agent_2 = Agent(
    name="Agent2",
    system_message="You are a helpful assistant.",
    llm_config=llm_config,
    keep_history=False
)

# Test queries that reference previous interactions
queries = [
    "What is the capital of France?",
    "What is the population of this city?",
    "Is this city bigger than London?"
]

if __name__ == "__main__":
    print("\nTesting Agent With History:")
    print("==========================")
    for query in queries:
        response = agent_1.chat(query)
        print(f"\nQuery: {query}")
        print(f"Response: {response}")

    print("\nTesting Agent Without History:")
    print("============================")
    for query in queries:
        response = agent_2.chat(query)
        print(f"\nQuery: {query}")
        print(f"Response: {response}")


================================================
FILE: examples/chat_history_display.py
================================================
"""
Example demonstrating comprehensive usage of the history management system
with various scenarios and detailed history viewing.
"""

import os, sys
from dotenv import load_dotenv
from typing import Dict, Any, List, Union

# Assuming primisai is in the Python path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from src.xronai.core import Agent, Supervisor
from openai.types.chat import ChatCompletionMessage

# Load environment variables
load_dotenv()


def message_to_dict(msg: Union[Dict[str, Any], ChatCompletionMessage]) -> Dict[str, Any]:
    """Convert a message to dictionary format regardless of its type."""
    if isinstance(msg, dict):
        return msg
    elif isinstance(msg, ChatCompletionMessage):
        message_dict = {'role': msg.role, 'content': msg.content}
        if hasattr(msg, 'tool_calls') and msg.tool_calls:
            message_dict['tool_calls'] = [{
                'id': tool_call.id,
                'function': {
                    'name': tool_call.function.name,
                    'arguments': tool_call.function.arguments
                }
            } for tool_call in msg.tool_calls]
        return message_dict
    else:
        raise ValueError(f"Unsupported message type: {type(msg)}")


def print_formatted_history(title: str, messages: List[Union[Dict[str, Any], ChatCompletionMessage]]) -> None:
    """Pretty print message history with clear formatting."""
    print("\n" + "=" * 50)
    print(f"{title}")
    print("=" * 50)

    for msg in messages:
        print("\n---Message---")
        msg_dict = message_to_dict(msg)

        print(f"Role: {msg_dict['role']}")

        # Format content based on message type
        if msg_dict['role'] == 'assistant' and 'tool_calls' in msg_dict:
            print("Content:", msg_dict['content'])
            print("Tool Calls:")
            for tool_call in msg_dict['tool_calls']:
                print(f"  - Function: {tool_call['function']['name']}")
                print(f"    Arguments: {tool_call['function']['arguments']}")
        elif msg_dict['role'] == 'tool':
            print(f"Content: {msg_dict['content']}")
            if 'tool_call_id' in msg_dict:
                print(f"Tool Call ID: {msg_dict['tool_call_id']}")
        else:
            print(f"Content: {msg_dict['content']}")
    print("\n" + "=" * 50 + "\n")


def main():
    # Configuration
    llm_config = {
        'model': os.getenv('LLM_MODEL'),
        'api_key': os.getenv('LLM_API_KEY'),
        'base_url': os.getenv('LLM_BASE_URL')
    }

    # Create supervisor and agents
    supervisor = Supervisor(name="MainSupervisor",
                            llm_config=llm_config,
                            system_message="You are a helpful supervisor coordinating multiple agents.")

    # Create specialized agents
    math_agent = Agent(name="MathAgent",
                       llm_config=llm_config,
                       system_message="You are a mathematical computation specialist.")

    writing_agent = Agent(name="WritingAgent",
                          llm_config=llm_config,
                          system_message="You are a creative writing specialist.")

    # Register agents
    supervisor.register_agent(math_agent)
    supervisor.register_agent(writing_agent)

    # Scenario 1: Direct supervisor response
    print("\nScenario 1: Direct supervisor response")
    query = "What's your name?"
    response = supervisor.chat(query)
    print(f"User: {query}")
    print(f"Supervisor: {response}")
    print_formatted_history("Supervisor Direct Response History", supervisor.get_chat_history())

    # Scenario 2: Single agent delegation
    print("\nScenario 2: Single agent delegation")
    query = "Calculate 15 + 27"
    response = supervisor.chat(query)
    print(f"User: {query}")
    print(f"Final Response: {response}")
    print_formatted_history("Supervisor History with Agent Delegation", supervisor.get_chat_history())
    print_formatted_history("Math Agent History", math_agent.get_chat_history())

    # Scenario 3: Multiple agent interaction
    print("\nScenario 3: Multiple agent interaction")
    query = """I need two things:
    1. Calculate 25 * 4
    2. Write a haiku about mathematics"""
    response = supervisor.chat(query)
    print(f"User: {query}")
    print(f"Final Response: {response}")

    print("\nFull conversation history after multiple interactions:")
    print("\nSupervisor History:")
    print_formatted_history("Complete Supervisor History", supervisor.get_chat_history())

    print("\nMath Agent History:")
    print_formatted_history("Math Agent History", math_agent.get_chat_history())

    print("\nWriting Agent History:")
    print_formatted_history("Writing Agent History", writing_agent.get_chat_history())

    # Scenario 4: Agent with tools
    calculator_metadata = {
        "type": "function",
        "function": {
            "name": "calculate",
            "description": "Perform basic arithmetic",
            "parameters": {
                "type": "object",
                "properties": {
                    "operation": {
                        "type": "string",
                        "enum": ["add", "subtract", "multiply", "divide"]
                    },
                    "numbers": {
                        "type": "array",
                        "items": {
                            "type": "number"
                        }
                    }
                },
                "required": ["operation", "numbers"]
            }
        }
    }

    def calculator(operation: str, numbers: List[float]) -> float:
        if operation == "add":
            return sum(numbers)
        elif operation == "multiply":
            result = 1
            for num in numbers:
                result *= num
            return result
        raise ValueError(f"Unsupported operation: {operation}")

    # Create new agent with tool
    calc_agent = Agent(name="CalculatorAgent",
                       llm_config=llm_config,
                       system_message="You are a calculator agent with built-in computation tools.",
                       tools=[{
                           "tool": calculator,
                           "metadata": calculator_metadata
                       }],
                       use_tools=True)
    supervisor.register_agent(calc_agent)

    query = "Calculate the product of 5, 3, and 4"
    response = supervisor.chat(query)
    print(f"\nScenario 4: Tool-enabled agent")
    print(f"User: {query}")
    print(f"Final Response: {response}")

    print("\nSupervisor History with Tool-enabled Agent:")
    print_formatted_history("Supervisor History", supervisor.get_chat_history())

    print("\nCalculator Agent History:")
    print_formatted_history("Calculator Agent History", calc_agent.get_chat_history())

    query = "How many agents you have? tell name of every agent."
    response = supervisor.chat(query)
    print(f"\nScenario 5: Tool-enabled agent")
    print(f"User: {query}")
    print(f"Final Response: {response}")


if __name__ == "__main__":
    main()



================================================
FILE: examples/eda.py
================================================
import sys, os, subprocess
from dotenv import load_dotenv

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from src.xronai.core import Agent, Supervisor

load_dotenv()

llm_config = {
    'model': os.getenv('LLM_MODEL'),
    'api_key': os.getenv('LLM_API_KEY'),
    'base_url': os.getenv('LLM_BASE_URL')
}


def execute_command(argument: str):
    try:
        result = subprocess.run(argument, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        output = result.stdout + result.stderr
        if result.returncode == 0:
            return {"status": "success", "output": output.strip()}
        else:
            return {"status": "error", "output": output.strip()}
    except Exception as e:
        return {"status": "error", "output": str(e)}


function_metadata = {
    "type": "function",
    "function": {
        "name":
            "execute_command",
        "description":
            "Execute a command on the Ubuntu terminal and capture the output or error. This function is called when user wants to execute any command on terminal",
        "parameters": {
            "type": "object",
            "properties": {
                "argument": {
                    "type": "string",
                    "description": "Command to execute on the Ubuntu terminal."
                }
            },
            "required": ["argument"]
        }
    }
}

tools = [{"tool": execute_command, "metadata": function_metadata}]

# Initialize agents
planner = Agent(
    name="Planner",
    system_message=
    "You are a professional Verilog test planner who writes test plans when users ask for code. Don't assume; ask for complete information if it's missing.",
    llm_config=llm_config)

coder = Agent(name="Coder",
              system_message=
              "You are a professional Verilog programmer. Don't assume; ask for complete information if it's missing.",
              llm_config=llm_config)

debugger = Agent(
    name="Debugger",
    system_message=
    "You are a professional debugger programmer. Don't assume; ask for complete information if it's missing. You can access terminal to run iverilog to test code syntax and functionality",
    llm_config=llm_config,
    tools=tools,
    use_tools=True)

# Initialize supervisor
supervisor = Supervisor(name="Supervisor",
                        system_message="Think you are a hardware design center manager who controls other agents.",
                        llm_config=llm_config)

supervisor.register_agent(planner)
supervisor.register_agent(coder)
supervisor.register_agent(debugger)

supervisor.display_agent_graph()
supervisor.start_interactive_session()



================================================
FILE: examples/hierarchical_structure.py
================================================
import os, sys
from dotenv import load_dotenv

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from src.xronai.core import Agent, Supervisor

# Load environment variables
load_dotenv()

# Configuration
llm_config = {
    'model': os.getenv('LLM_MODEL'),
    'api_key': os.getenv('LLM_API_KEY'),
    'base_url': os.getenv('LLM_BASE_URL')
}


def test_hierarchical_structure():
    # Create agents
    agent1 = Agent(name="Agent1", llm_config=llm_config, system_message="You are Agent1.")
    agent2 = Agent(name="Agent2", llm_config=llm_config, system_message="You are Agent2.")
    agent3 = Agent(name="Agent3", llm_config=llm_config, system_message="You are Agent3.")

    # Create sub-supervisor
    sub_supervisor = Supervisor(name="SubSupervisor",
                                system_message="You are a sub-supervisor managing Agent2 and Agent3.",
                                llm_config=llm_config,
                                is_assistant=True)
    sub_supervisor.register_agent(agent2)
    sub_supervisor.register_agent(agent3)

    # Create main supervisor
    main_supervisor = Supervisor(name="MainSupervisor",
                                 system_message="You are the main supervisor managing Agent1 and SubSupervisor.",
                                 llm_config=llm_config)
    main_supervisor.register_agent(agent1)
    main_supervisor.register_agent(sub_supervisor)

    # Test the structure
    print("Testing hierarchical structure:")
    print(f"Main Supervisor: {main_supervisor.name}")
    print(f"Registered agents with Main Supervisor: {main_supervisor.get_registered_agents()}")
    print(f"Sub-Supervisor: {sub_supervisor.name}")
    print(f"Registered agents with Sub-Supervisor: {sub_supervisor.get_registered_agents()}")

    # Test chat functionality
    test_query = "Hello, can you demonstrate the hierarchical structure?"
    print(f"\nTesting chat with query: '{test_query}'")
    response = main_supervisor.chat(test_query)
    print(f"Response: {response}")

    # Display the agent graph
    print("\nAgent Graph:")
    main_supervisor.display_agent_graph()


if __name__ == "__main__":
    test_hierarchical_structure()



================================================
FILE: examples/history_loading.py
================================================
import os
import sys
from dotenv import load_dotenv

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from src.xronai.core import Agent, Supervisor
from src.xronai.history import HistoryManager

# Load environment variables
load_dotenv()

# LLM Configuration
llm_config = {
    'model': os.getenv('LLM_MODEL'),
    'api_key': os.getenv('LLM_API_KEY'),
    'base_url': os.getenv('LLM_BASE_URL')
}


def create_calculator_agent():
    """Create a calculator agent with tools."""
    calculator_metadata = {
        "type": "function",
        "function": {
            "name": "calculate",
            "description": "Perform basic arithmetic operations.",
            "parameters": {
                "type": "object",
                "properties": {
                    "num1": {
                        "type": "integer",
                        "description": "First number"
                    },
                    "num2": {
                        "type": "integer",
                        "description": "Second number"
                    },
                    "operation": {
                        "type": "string",
                        "description": "Mathematical operation to perform",
                        "enum": ["add", "subtract", "multiply", "divide"]
                    }
                },
                "required": ["num1", "num2", "operation"]
            }
        }
    }

    def calculator(num1: int, num2: int, operation: str) -> float:
        if operation == "add":
            return num1 + num2
        elif operation == "subtract":
            return num1 - num2
        elif operation == "multiply":
            return num1 * num2
        elif operation == "divide":
            return num1 / num2
        else:
            raise ValueError("Invalid operation")

    return Agent(name="Calculator_Agent",
                 llm_config=llm_config,
                 system_message="You are a mathematical calculator agent.",
                 tools=[{
                     "tool": calculator,
                     "metadata": calculator_metadata
                 }],
                 use_tools=True)


def create_text_agent():
    """Create a text processing agent with tools."""
    text_processor_metadata = {
        "type": "function",
        "function": {
            "name": "process_text",
            "description": "Process text with various operations.",
            "parameters": {
                "type": "object",
                "properties": {
                    "text": {
                        "type": "string",
                        "description": "Text to process"
                    },
                    "operation": {
                        "type": "string",
                        "description": "Text operation to perform",
                        "enum": ["uppercase", "lowercase", "reverse"]
                    }
                },
                "required": ["text", "operation"]
            }
        }
    }

    def text_processor(text: str, operation: str) -> str:
        if operation == "uppercase":
            return text.upper()
        elif operation == "lowercase":
            return text.lower()
        elif operation == "reverse":
            return text[::-1]
        else:
            raise ValueError("Invalid operation")

    return Agent(name="Text_Processor_Agent",
                 llm_config=llm_config,
                 system_message="You are a text processing agent.",
                 tools=[{
                     "tool": text_processor,
                     "metadata": text_processor_metadata
                 }],
                 use_tools=True)


def test_history_management():
    """Test history management functionality."""
    print("\n=== Testing History Management ===\n")

    # Create a new supervisor and agents
    supervisor = Supervisor(name="History_Test_Supervisor",
                            llm_config=llm_config,
                            system_message="You are a supervisor managing calculation and text processing tasks.")

    calc_agent = create_calculator_agent()
    text_agent = create_text_agent()

    supervisor.register_agent(calc_agent)
    supervisor.register_agent(text_agent)

    # supervisor.display_agent_graph()

    # Display initial structure
    print("Initial Agent Structure:")
    supervisor.display_agent_graph()
    print("\n")

    # Test 1: Initial interaction
    print("Test 1: Initial interaction")
    query = "Add 15 and 27, then convert 'Hello World' to uppercase"
    print(f"Query: {query}")
    response = supervisor.chat(query)
    print(f"Response: {response}\n")

    # Store workflow_id for later use
    workflow_id = supervisor.workflow_id

    # Test 2: Create new supervisor with same workflow_id and load history
    print("Test 2: Loading history into new supervisor instance")
    new_supervisor = Supervisor(name="History_Test_Supervisor",
                                llm_config=llm_config,
                                system_message="You are a supervisor managing calculation and text processing tasks.",
                                workflow_id=workflow_id)

    # Initialize history manager
    history_manager = HistoryManager(workflow_id)

    # Create new agents
    new_calc_agent = create_calculator_agent()
    new_text_agent = create_text_agent()

    # Register agents
    new_supervisor.register_agent(new_calc_agent)
    new_supervisor.register_agent(new_text_agent)

    # Load histories
    new_supervisor.chat_history = history_manager.load_chat_history("History_Test_Supervisor")
    new_calc_agent.chat_history = history_manager.load_chat_history("Calculator_Agent")
    new_text_agent.chat_history = history_manager.load_chat_history("Text_Processor_Agent")

    print("History_Test_Supervisor: \n", new_supervisor.chat_history)
    print("\nCalculator_Agent: \n", new_calc_agent.chat_history)
    print("\nText_Processor_Agent: \n", new_text_agent.chat_history)

    # Display loaded histories
    print("\nLoaded Chat Histories:")

    print("\nSupervisor History:")
    for msg in new_supervisor.chat_history:
        print(f"Role: {msg['role']}, "
              f"Content: {msg.get('content', 'None')}, "
              f"Tool Calls: {'Yes' if 'tool_calls' in msg else 'No'}")

    print("\nCalculator Agent History:")
    for msg in new_calc_agent.chat_history:
        print(f"Role: {msg['role']}, "
              f"Content: {msg.get('content', 'None')}")

    print("\nText Agent History:")
    for msg in new_text_agent.chat_history:
        print(f"Role: {msg['role']}, "
              f"Content: {msg.get('content', 'None')}")

    # Test 3: Continue conversation with loaded history
    print("\nTest 3: Continuing conversation with loaded history")

    try:
        follow_up_query = "What was the result of the previous addition?"
        print(f"Follow-up Query: {follow_up_query}")
        response = new_supervisor.chat(follow_up_query)
        print(f"Response: {response}\n")

        follow_up_query = "Add 17 and 90"
        print(f"Follow-up Query: {follow_up_query}")
        response = new_supervisor.chat(follow_up_query)
        print(f"Response: {response}\n")

    except Exception as e:
        print(f"Error in continuation: {str(e)}\n")


def main():
    try:
        test_history_management()
    except Exception as e:
        print(f"Error occurred: {str(e)}")
        raise e


if __name__ == "__main__":
    main()



================================================
FILE: examples/history_loading_with_assistant_supervisor.py
================================================
import os, sys
from dotenv import load_dotenv

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from src.xronai.core import Agent, Supervisor
from src.xronai.history import HistoryManager

# Load environment variables
load_dotenv()

# Configuration
llm_config = {
    'model': os.getenv('LLM_MODEL'),
    'api_key': os.getenv('LLM_API_KEY'),
    'base_url': os.getenv('LLM_BASE_URL')
}


def test_hierarchical_structure():
    # Create agents
    agent1 = Agent(name="Agent1", llm_config=llm_config, system_message="You are Agent1.")
    agent2 = Agent(name="Agent2", llm_config=llm_config, system_message="You are Agent2.")
    agent3 = Agent(name="Agent3", llm_config=llm_config, system_message="You are Agent3.")

    # Create sub-supervisor
    sub_supervisor = Supervisor(name="SubSupervisor",
                                system_message="You are a sub-supervisor managing Agent2 and Agent3.",
                                llm_config=llm_config,
                                is_assistant=True)
    sub_supervisor.register_agent(agent2)
    sub_supervisor.register_agent(agent3)

    # Create main supervisor
    main_supervisor = Supervisor(name="MainSupervisor",
                                 system_message="You are the main supervisor managing Agent1 and SubSupervisor.",
                                 llm_config=llm_config)
    main_supervisor.register_agent(agent1)
    main_supervisor.register_agent(sub_supervisor)

    # Test the structure
    print("Testing hierarchical structure:")
    print(f"Main Supervisor: {main_supervisor.name}")
    print(f"Registered agents with Main Supervisor: {main_supervisor.get_registered_agents()}")
    print(f"Sub-Supervisor: {sub_supervisor.name}")
    print(f"Registered agents with Sub-Supervisor: {sub_supervisor.get_registered_agents()}")

    # Test chat functionality
    test_query = "Hello, can you demonstrate the hierarchical structure by asking all the agents?"
    print(f"\nTesting chat with query: '{test_query}'")
    response = main_supervisor.chat(test_query)
    print(f"Response: {response}")

    # Display the agent graph
    print("\nAgent Graph:")
    main_supervisor.display_agent_graph()

    workflow_id = main_supervisor.workflow_id
    history_manager = HistoryManager(workflow_id)

    print("MainSupervisor: \n", history_manager.load_chat_history("MainSupervisor"))
    print("\nSubSupervisor: \n", history_manager.load_chat_history("SubSupervisor"))
    print("\nAgent1: \n", history_manager.load_chat_history("Agent1"))
    print("\nAgent2: \n", history_manager.load_chat_history("Agent2"))
    print("\nAgent3: \n", history_manager.load_chat_history("Agent3"))


if __name__ == "__main__":
    test_hierarchical_structure()



================================================
FILE: examples/multi_argument_function_calling.py
================================================
import os, sys
from dotenv import load_dotenv

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from src.xronai.core import Agent

# Load environment variables
load_dotenv()

# LLM Configuration
llm_config = {
    'model': os.getenv('LLM_MODEL'),
    'api_key': os.getenv('LLM_API_KEY'),
    'base_url': os.getenv('LLM_BASE_URL')
}

# Test tool metadata
test_tool_metadata = {
    "type": "function",
    "function": {
        "name": "test_tool",
        "description": "Simple sum test function.",
        "parameters": {
            "type": "object",
            "properties": {
                "num1": {
                    "type": "integer",
                    "description": "First integer"
                },
                "num2": {
                    "type": "integer",
                    "description": "Second integer"
                },
                "num3": {
                    "type": "integer",
                    "description": "Third integer"
                }
            },
            "required": ["num1", "num2", "num3"]
        }
    }
}


# Test tool function
def test_tool_func(num1: int, num2: int, num3: int):
    return num1 + num2 + num3


# Combine function and metadata
test_tool = [{"tool": test_tool_func, "metadata": test_tool_metadata}]

# Initialize test agent
test_agent = Agent(name="Test Agent",
                   system_message="You are the Test Agent.",
                   llm_config=llm_config,
                   tools=test_tool,
                   use_tools=True)

# Test the agent
if __name__ == "__main__":
    query = "Can you run test tool with input value 4, 6 and 5?"
    response = test_agent.chat(query)
    print(f"Query: {query}")
    print(f"Response: {response}")



================================================
FILE: examples/multi_logger.py
================================================
import os, sys
from dotenv import load_dotenv

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from src.xronai.core import Agent, Supervisor

# Load environment variables
load_dotenv()

# LLM Configuration
llm_config = {
    'model': os.getenv('LLM_MODEL'),
    'api_key': os.getenv('LLM_API_KEY'),
    'base_url': os.getenv('LLM_BASE_URL')
}

# Tool 1: Calculator
calculator_metadata = {
    "type": "function",
    "function": {
        "name": "calculate",
        "description": "Perform basic arithmetic operations.",
        "parameters": {
            "type": "object",
            "properties": {
                "num1": {
                    "type": "integer",
                    "description": "First number"
                },
                "num2": {
                    "type": "integer",
                    "description": "Second number"
                },
                "operation": {
                    "type": "string",
                    "description": "Mathematical operation to perform",
                    "enum": ["add", "subtract", "multiply", "divide"]
                }
            },
            "required": ["num1", "num2", "operation"]
        }
    }
}


def calculator(num1: int, num2: int, operation: str) -> float:
    if operation == "add":
        return num1 + num2
    elif operation == "subtract":
        return num1 - num2
    elif operation == "multiply":
        return num1 * num2
    elif operation == "divide":
        return num1 / num2
    else:
        raise ValueError("Invalid operation")


# Tool 2: Text Processor
text_processor_metadata = {
    "type": "function",
    "function": {
        "name": "process_text",
        "description": "Process text with various operations.",
        "parameters": {
            "type": "object",
            "properties": {
                "text": {
                    "type": "string",
                    "description": "Text to process"
                },
                "operation": {
                    "type": "string",
                    "description": "Text operation to perform",
                    "enum": ["uppercase", "lowercase", "reverse"]
                }
            },
            "required": ["text", "operation"]
        }
    }
}


def text_processor(text: str, operation: str) -> str:
    if operation == "uppercase":
        return text.upper()
    elif operation == "lowercase":
        return text.lower()
    elif operation == "reverse":
        return text[::-1]
    else:
        raise ValueError("Invalid operation")


def create_supervisor():

    # Create agents with their respective tools
    calculator_agent = Agent(name="Calculator_Agent",
                             llm_config=llm_config,
                             system_message="You are a mathematical calculator agent.",
                             tools=[{
                                 "tool": calculator,
                                 "metadata": calculator_metadata
                             }],
                             use_tools=True)

    text_agent = Agent(name="Text_Processor_Agent",
                       llm_config=llm_config,
                       system_message="You are a text processing agent.",
                       tools=[{
                           "tool": text_processor,
                           "metadata": text_processor_metadata
                       }],
                       use_tools=True)

    # Create supervisor
    supervisor = Supervisor(name="Multi_Tool_Supervisor",
                            llm_config=llm_config,
                            system_message="""You are a helpful supervisor who can assign tasks to agents.""")

    # Register agents with supervisor
    supervisor.register_agent(calculator_agent)
    supervisor.register_agent(text_agent)

    return supervisor


if __name__ == "__main__":

    for i in range(1, 3):
        # Use this example query:  I need three things done: 1. Add 15 and 27 2. Convert 'Hello World' to uppercase 3. Multiply 8 by 6
        supervisor = create_supervisor()
        supervisor.start_interactive_session()



================================================
FILE: examples/multiple_tool_calls.py
================================================
import os, sys
from dotenv import load_dotenv

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from src.xronai.core import Agent, Supervisor

# Load environment variables
load_dotenv()

# LLM Configuration
llm_config = {
    'model': os.getenv('LLM_MODEL'),
    'api_key': os.getenv('LLM_API_KEY'),
    'base_url': os.getenv('LLM_BASE_URL')
}

# Tool 1: Calculator
calculator_metadata = {
    "type": "function",
    "function": {
        "name": "calculate",
        "description": "Perform basic arithmetic operations.",
        "parameters": {
            "type": "object",
            "properties": {
                "num1": {
                    "type": "integer",
                    "description": "First number"
                },
                "num2": {
                    "type": "integer",
                    "description": "Second number"
                },
                "operation": {
                    "type": "string",
                    "description": "Mathematical operation to perform",
                    "enum": ["add", "subtract", "multiply", "divide"]
                }
            },
            "required": ["num1", "num2", "operation"]
        }
    }
}


def calculator(num1: int, num2: int, operation: str) -> float:
    if operation == "add":
        return num1 + num2
    elif operation == "subtract":
        return num1 - num2
    elif operation == "multiply":
        return num1 * num2
    elif operation == "divide":
        return num1 / num2
    else:
        raise ValueError("Invalid operation")


# Tool 2: Text Processor
text_processor_metadata = {
    "type": "function",
    "function": {
        "name": "process_text",
        "description": "Process text with various operations.",
        "parameters": {
            "type": "object",
            "properties": {
                "text": {
                    "type": "string",
                    "description": "Text to process"
                },
                "operation": {
                    "type": "string",
                    "description": "Text operation to perform",
                    "enum": ["uppercase", "lowercase", "reverse"]
                }
            },
            "required": ["text", "operation"]
        }
    }
}


def text_processor(text: str, operation: str) -> str:
    if operation == "uppercase":
        return text.upper()
    elif operation == "lowercase":
        return text.lower()
    elif operation == "reverse":
        return text[::-1]
    else:
        raise ValueError("Invalid operation")


# Create agents with their respective tools
calculator_agent = Agent(name="Calculator_Agent",
                         llm_config=llm_config,
                         system_message="You are a mathematical calculator agent.",
                         tools=[{
                             "tool": calculator,
                             "metadata": calculator_metadata
                         }],
                         use_tools=True)

text_agent = Agent(name="Text_Processor_Agent",
                   llm_config=llm_config,
                   system_message="You are a text processing agent.",
                   tools=[{
                       "tool": text_processor,
                       "metadata": text_processor_metadata
                   }],
                   use_tools=True)

# Create supervisor
supervisor = Supervisor(name="Multi_Tool_Supervisor",
                        llm_config=llm_config,
                        system_message="""You are a helpful supervisor who can assign tasks to agents.""")

# Register agents with supervisor
supervisor.register_agent(calculator_agent)
supervisor.register_agent(text_agent)

supervisor.display_agent_graph()

if __name__ == "__main__":
    # Example query that requires multiple tool calls
    query = """I need three things done: 1. Add 15 and 27 2. Convert 'Hello World' to uppercase 3. Multiply 8 by 6"""

    print(f"Query: {query}\n")
    response = supervisor.chat(query)
    print(f"Final Response: {response}")



================================================
FILE: examples/output_schema_examples.py
================================================
"""
Example demonstrating the Agent output schema functionality.
This example shows:
1. Basic schema usage
2. Strict vs non-strict mode
3. Different schema types (code, analysis, structured data)
4. Schema validation and reformatting
"""

import os, sys
import json
from pprint import pprint
from dotenv import load_dotenv

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from src.xronai.core import Agent

# Load environment variables
load_dotenv()

# LLM Configuration
llm_config = {
    'model': os.getenv('LLM_MODEL'),
    'api_key': os.getenv('LLM_API_KEY'),
    'base_url': os.getenv('LLM_BASE_URL')
}


def print_json_response(response: str, title: str = None):
    """Helper function to pretty print JSON responses"""
    if title:
        print(f"\n=== {title} ===")
    try:
        # Try to parse and pretty print as JSON
        parsed = json.loads(response)
        print("\nFormatted Response:")
        pprint(parsed, indent=2, width=80)
    except json.JSONDecodeError:
        # If not JSON, print as is
        print("\nPlain Response:")
        print(response)
    print("\n" + "=" * 50)


# Example 1: Code Generation Agent with Schema
code_schema = {
    "type": "object",
    "properties": {
        "description": {
            "type": "string",
            "description": "Explanation of the code's purpose and functionality"
        },
        "code": {
            "type": "string",
            "description": "The actual code implementation"
        },
        "language": {
            "type": "string",
            "description": "Programming language used"
        }
    },
    "required": ["description", "code"]
}

code_agent = Agent(
    name="CodeWriter",
    llm_config=llm_config,
    system_message="You are a skilled programmer who writes clean, well-documented code.",
    output_schema=code_schema,
    strict=True  # Enforce JSON schema
)

# Example 2: Analysis Agent with Schema (non-strict)
analysis_schema = {
    "type": "object",
    "properties": {
        "summary": {
            "type": "string",
            "description": "Brief summary of the analysis"
        },
        "key_points": {
            "type": "array",
            "items": {
                "type": "string"
            },
            "description": "List of key findings or points"
        },
        "recommendations": {
            "type": "array",
            "items": {
                "type": "string"
            },
            "description": "List of recommendations based on analysis"
        }
    },
    "required": ["summary", "key_points"]
}

analysis_agent = Agent(
    name="Analyst",
    llm_config=llm_config,
    system_message="You are an analytical expert who provides detailed insights.",
    output_schema=analysis_schema,
    strict=False  # Allow non-JSON responses if parsing fails
)

# Example 3: Regular Agent (no schema)
regular_agent = Agent(name="Assistant", llm_config=llm_config, system_message="You are a helpful assistant.")


def test_code_agent():
    print("\n=== Testing Code Agent (Strict Schema) ===")
    query = "Write a Python function that calculates the factorial of a number."
    print(f"\nQuery: {query}")
    response = code_agent.chat(query)
    print_json_response(response, "Code Agent Response")


def test_analysis_agent():
    print("\n=== Testing Analysis Agent (Non-strict Schema) ===")
    query = "Analyze the advantages and disadvantages of remote work."
    print(f"\nQuery: {query}")
    response = analysis_agent.chat(query)
    print_json_response(response, "Analysis Agent Response")


def test_regular_agent():
    print("\n=== Testing Regular Agent (No Schema) ===")
    query = "What are the benefits of exercise?"
    print(f"\nQuery: {query}")
    response = regular_agent.chat(query)
    print_json_response(response, "Regular Agent Response")


def test_schema_enforcement():
    print("\n=== Testing Schema Enforcement ===")

    # Test 1: Complex query that might challenge schema compliance
    print("\nTest 1: Complex Query (Code Agent)")
    query = "Explain object-oriented programming and provide an example class."
    print(f"\nQuery: {query}")
    response = code_agent.chat(query)
    print_json_response(response, "Code Agent (Complex Query)")

    # Test 2: Query that might produce non-schema response
    print("\nTest 2: Potentially Non-Schema Response (Analysis Agent)")
    query = "Quick overview of climate change?"
    print(f"\nQuery: {query}")
    response = analysis_agent.chat(query)
    print_json_response(response, "Analysis Agent (Quick Overview)")


def display_schema_info():
    """Display the schemas being used in the example"""
    print("\n=== Schema Definitions ===")
    print("\nCode Agent Schema:")
    pprint(code_schema, indent=2, width=80)
    print("\nAnalysis Agent Schema:")
    pprint(analysis_schema, indent=2, width=80)
    print("\n" + "=" * 50)


def main():
    print("=== Output Schema Examples ===")
    print("This example demonstrates different ways to use output schemas with agents.")

    # Display schema definitions
    display_schema_info()

    # Test each agent type
    test_code_agent()
    test_analysis_agent()
    test_regular_agent()

    # Test schema enforcement
    test_schema_enforcement()


if __name__ == "__main__":
    main()



================================================
FILE: examples/schema_aware_workflow_example.py
================================================
"""
Example demonstrating output schema functionality in a workflow with supervisor and agents.
This example simulates a software documentation workflow where:
1. A code analysis agent examines code (with structured output)
2. A documentation writer creates docs (with template schema)
3. A quality checker reviews (with checklist schema)
"""

import os, sys
import json
from pprint import pprint
from dotenv import load_dotenv

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from src.xronai.core import Agent, Supervisor

# Load environment variables
load_dotenv()

# LLM Configuration
llm_config = {
    'model': os.getenv('LLM_MODEL'),
    'api_key': os.getenv('LLM_API_KEY'),
    'base_url': os.getenv('LLM_BASE_URL')
}


def print_json_response(response: str, title: str = None):
    """Helper function to pretty print JSON responses"""
    if title:
        print(f"\n=== {title} ===")
    try:
        parsed = json.loads(response)
        print("\nFormatted Response:")
        pprint(parsed, indent=2, width=80)
    except json.JSONDecodeError:
        print("\nPlain Response:")
        print(response)
    print("\n" + "=" * 50)


# Define schemas for different agents
code_analyzer_schema = {
    "type": "object",
    "properties": {
        "complexity_score": {
            "type": "integer",
            "description": "Code complexity score (1-10)"
        },
        "key_components": {
            "type": "array",
            "items": {
                "type": "string"
            },
            "description": "List of main code components identified"
        },
        "potential_issues": {
            "type": "array",
            "items": {
                "type": "string"
            },
            "description": "List of potential issues or improvements"
        }
    },
    "required": ["complexity_score", "key_components"]
}

doc_writer_schema = {
    "type": "object",
    "properties": {
        "title": {
            "type": "string",
            "description": "Documentation title"
        },
        "overview": {
            "type": "string",
            "description": "Brief overview of the code/feature"
        },
        "usage_examples": {
            "type": "array",
            "items": {
                "type": "string"
            },
            "description": "Example usage snippets"
        },
        "api_documentation": {
            "type": "object",
            "description": "API details",
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "return_value": {
                    "type": "string"
                }
            }
        }
    },
    "required": ["title", "overview", "usage_examples"]
}

quality_checker_schema = {
    "type": "object",
    "properties": {
        "passes_checklist": {
            "type": "boolean",
            "description": "Whether documentation passes all checks"
        },
        "checklist_results": {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "check": {
                        "type": "string"
                    },
                    "passed": {
                        "type": "boolean"
                    },
                    "comments": {
                        "type": "string"
                    }
                }
            }
        },
        "improvement_suggestions": {
            "type": "array",
            "items": {
                "type": "string"
            }
        }
    },
    "required": ["passes_checklist", "checklist_results"]
}

# Create agents with schemas
code_analyzer = Agent(name="CodeAnalyzer",
                      llm_config=llm_config,
                      system_message="You are an expert code analyzer. Examine code and provide structured analysis.",
                      output_schema=code_analyzer_schema,
                      strict=True)

doc_writer = Agent(
    name="DocWriter",
    llm_config=llm_config,
    system_message="You are a technical documentation writer. Create clear, comprehensive documentation.",
    output_schema=doc_writer_schema,
    strict=True)

quality_checker = Agent(name="QualityChecker",
                        llm_config=llm_config,
                        system_message="You are a documentation quality checker. Ensure docs meet all standards.",
                        output_schema=quality_checker_schema,
                        strict=True)

# Create and configure supervisor
doc_supervisor = Supervisor(name="DocSupervisor",
                            llm_config=llm_config,
                            system_message="""You are a documentation project supervisor. 
    Coordinate between the code analyzer, documentation writer, and quality checker.
    Follow this workflow:
    1. Have code analyzed first
    2. Based on analysis, request documentation
    3. Finally, check documentation quality""")

# Register agents
doc_supervisor.register_agent(code_analyzer)
doc_supervisor.register_agent(doc_writer)
doc_supervisor.register_agent(quality_checker)


def display_workflow_structure():
    """Display the workflow structure and schemas"""
    print("\n=== Workflow Structure ===")
    doc_supervisor.display_agent_graph()

    print("\n=== Agent Schemas ===")
    print("\nCode Analyzer Schema:")
    pprint(code_analyzer_schema)
    print("\nDoc Writer Schema:")
    pprint(doc_writer_schema)
    print("\nQuality Checker Schema:")
    pprint(quality_checker_schema)
    print("\n" + "=" * 50)


def test_documentation_workflow():
    """Test the complete documentation workflow"""
    # Test case: Python function documentation
    python_code = """
def calculate_fibonacci(n):
    if n <= 0:
        return []
    elif n == 1:
        return [0]
    
    fib = [0, 1]
    for i in range(2, n):
        fib.append(fib[i-1] + fib[i-2])
    return fib
    """

    print("\n=== Testing Documentation Workflow ===")
    print("\nInput Code:")
    print(python_code)

    # First query: Request documentation for the code
    query = f"""Create documentation for this Python code:
    {python_code}
    
    Please analyze the code, create documentation, and verify quality."""

    print("\nInitial Query:", query)
    response = doc_supervisor.chat(query)
    print_json_response(response, "Supervisor's Final Response")

    # Follow-up query to demonstrate context awareness
    follow_up = "What were the main quality checks performed on this documentation?"
    print("\nFollow-up Query:", follow_up)
    response = doc_supervisor.chat(follow_up)
    print_json_response(response, "Supervisor's Follow-up Response")


def main():
    print("=== Documentation Workflow with Schema-Aware Agents ===")
    print("This example demonstrates a complete documentation workflow using")
    print("multiple agents with structured outputs managed by a supervisor.")

    # Display workflow structure and schemas
    display_workflow_structure()

    # Run the workflow test
    test_documentation_workflow()


if __name__ == "__main__":
    main()



================================================
FILE: examples/workflow_with_custom_id.py
================================================
"""
Simple workflow example demonstrating custom workflow_id with one supervisor and one agent.

This example shows:
1. How to create a workflow with a custom workflow_id
2. Basic interaction between supervisor and agent
3. Persistent storage across sessions
"""

import os
import sys
from dotenv import load_dotenv
from pathlib import Path

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from src.xronai.core import Agent, Supervisor

# Load environment variables
load_dotenv()

# LLM Configuration
llm_config = {
    'model': os.getenv('LLM_MODEL'),
    'api_key': os.getenv('LLM_API_KEY'),
    'base_url': os.getenv('LLM_BASE_URL')
}


def create_simple_workflow(workflow_id: str):
    """
    Create a simple workflow with custom workflow_id.
    
    Args:
        workflow_id (str): Custom workflow identifier
    
    Returns:
        Supervisor: The supervisor managing the workflow
    """
    print(f"Creating workflow: {workflow_id}")

    # Create supervisor with custom workflow_id
    supervisor = Supervisor(name="TaskSupervisor",
                            llm_config=llm_config,
                            workflow_id=workflow_id,
                            system_message="You are a helpful supervisor managing a writing assistant.")

    # Create a single agent
    writer_agent = Agent(name="WriterAgent",
                         llm_config=llm_config,
                         system_message="You are a creative writing assistant who helps with various writing tasks.")

    # Register the agent
    supervisor.register_agent(writer_agent)

    print(f"✓ Workflow '{workflow_id}' created successfully!")
    return supervisor


def main():
    """Main function to run the simple workflow example."""

    # Get workflow_id from user or use default
    if len(sys.argv) > 1:
        workflow_id = sys.argv[1]
    else:
        workflow_id = input("Enter workflow ID (or press Enter for 'simple_writing_workflow'): ").strip()
        if not workflow_id:
            workflow_id = "simple_writing_workflow"

    # Create the workflow
    supervisor = create_simple_workflow(workflow_id)

    # Show workflow structure
    print(f"\nWorkflow Structure:")
    supervisor.display_agent_graph()

    # Show workflow information
    print(f"\nWorkflow Information:")
    print(f"├── Workflow ID: {workflow_id}")
    print(f"├── Supervisor: TaskSupervisor")
    print(f"├── Agent: WriterAgent")
    print(f"└── Storage: nexus_workflows/{workflow_id}/")

    # Start interactive session
    print(f"\nStarting interactive session with workflow '{workflow_id}'")
    print("Type 'exit' to quit, 'info' to see workflow details")
    print("-" * 50)

    while True:
        user_input = input(f"\n[{workflow_id}] You: ").strip()

        if user_input.lower() == 'exit':
            print(f"Session ended. Workflow '{workflow_id}' has been saved.")
            break
        elif user_input.lower() == 'info':
            print(f"\nWorkflow: {workflow_id}")
            print(f"Messages in history: {len(supervisor.chat_history)}")
            supervisor.display_agent_graph()
        elif user_input:
            try:
                response = supervisor.chat(user_input)
                print(f"TaskSupervisor: {response}")
            except Exception as e:
                print(f"Error: {str(e)}")


if __name__ == "__main__":
    main()



================================================
FILE: examples/config_mcp_schema/config.yaml
================================================
workflow_id: "complete_workflow_example"

supervisor:
  name: MainSupervisor
  type: supervisor
  llm_config:
    model: ${LLM_MODEL}
    api_key: ${LLM_API_KEY}
    base_url: ${LLM_BASE_URL}
  system_message: "You are the main supervisor coordinating specialized agents."
  children:
    - name: CodeWriter
      type: agent
      llm_config:
        model: ${LLM_MODEL}
        api_key: ${LLM_API_KEY}
        base_url: ${LLM_BASE_URL}
      system_message: "You are a Python code writer."
      keep_history: true
      strict: true
      output_schema:
        type: "object"
        properties:
          description:
            type: "string"
            description: "Explanation of the code"
          code:
            type: "string"
            description: "The actual code implementation"
          language:
            type: "string"
            description: "Programming language used"
        required: ["description", "code"]
      
    - name: WeatherAgent
      type: agent
      llm_config:
        model: ${LLM_MODEL}
        api_key: ${LLM_API_KEY}
        base_url: ${LLM_BASE_URL}
      system_message: "You provide weather information."
      use_tools: true
      keep_history: true
      mcp_servers:
        - type: stdio
          script_path: "examples/supervisor_multi_mcp/weather_server.py"
      
    - name: AnalysisManager
      type: supervisor
      is_assistant: true
      llm_config:
        model: ${LLM_MODEL}
        api_key: ${LLM_API_KEY}
        base_url: ${LLM_BASE_URL}
      system_message: "You manage data analysis tasks."
      children:
        - name: DataAnalyst
          type: agent
          llm_config:
            model: ${LLM_MODEL}
            api_key: ${LLM_API_KEY}
            base_url: ${LLM_BASE_URL}
          system_message: "You analyze data and provide insights."
          strict: true
          output_schema:
            type: "object"
            properties:
              summary:
                type: "string"
                description: "Brief analysis summary"
              key_points:
                type: "array"
                items:
                  type: "string"
              recommendations:
                type: "array"
                items:
                  type: "string"
            required: ["summary", "key_points"]


================================================
FILE: examples/config_mcp_schema/config_mcp_schema.py
================================================
"""
Example demonstrating complete configuration capabilities including:
- Output schemas
- MCP servers
- Strict mode
- Hierarchical structure
"""

import os, sys
from pathlib import Path
from dotenv import load_dotenv

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

from src.xronai.config import load_yaml_config, AgentFactory

# Load environment variables
load_dotenv()


def print_json_response(response: str, title: str = None):
    """Helper function to pretty print JSON responses"""
    import json
    from pprint import pprint

    if title:
        print(f"\n=== {title} ===")
    try:
        parsed = json.loads(response)
        print("\nFormatted Response:")
        pprint(parsed, indent=2, width=80)
    except json.JSONDecodeError:
        print("\nPlain Response:")
        print(response)
    print("\n" + "=" * 50)


def ensure_workflow_directory(workflow_id: str):
    """Ensure workflow directory exists"""
    workflow_path = Path("xronai_logs") / workflow_id
    workflow_path.mkdir(parents=True, exist_ok=True)
    history_file = workflow_path / "history.jsonl"
    if not history_file.exists():
        history_file.touch()


def main():
    # Load configuration
    config = load_yaml_config('examples/config_mcp_schema/config.yaml')

    # Ensure workflow directory exists
    workflow_id = config.get('workflow_id', 'complete_workflow_example')
    ensure_workflow_directory(workflow_id)

    # Create supervisor and agents
    supervisor = AgentFactory.create_from_config(config)

    print("\n=== Complete Configuration Example ===")
    print("This example demonstrates all configuration capabilities")
    print("\nWorkflow Structure:")
    supervisor.display_agent_graph()

    # Test CodeWriter with schema
    print("\nTesting CodeWriter (with schema):")
    query = "Write a function to calculate the fibonacci sequence"
    print(f"Query: {query}")
    response = supervisor.chat(query)
    print_json_response(response)

    # Test WeatherAgent with MCP
    print("\nTesting WeatherAgent (with MCP):")
    query = "What's the weather forecast for New York?"
    print(f"Query: {query}")
    response = supervisor.chat(query)
    print_json_response(response)

    # Test DataAnalyst with schema
    print("\nTesting DataAnalyst (with schema):")
    query = "Analyze the trends in recent stock market data"
    print(f"Query: {query}")
    response = supervisor.chat(query)
    print_json_response(response)


if __name__ == "__main__":
    main()



================================================
FILE: examples/memory_task_management/config.yaml
================================================
supervisor:
  name: ContextTaskManager
  type: supervisor
  llm_config:
    model: ${LLM_MODEL}
    api_key: ${LLM_API_KEY}
    base_url: ${LLM_BASE_URL}
  system_message: "You are the context-aware task management supervisor. Coordinate between agents to manage tasks in different contexts."
  children:
    - name: StatefulTaskManager
      type: agent
      llm_config:
        model: ${LLM_MODEL}
        api_key: ${LLM_API_KEY}
        base_url: ${LLM_BASE_URL}
      system_message: "You are a stateful task manager that remembers context from previous interactions."
      keep_history: true
      tools:
        - name: add_task_with_context
          type: function
          python_path: examples.memory_task_management.task_tools.add_task_with_context
          description: "Add a task with context information"
          parameters:
            task:
              type: string
              description: "The task to be added"
            context:
              type: string
              description: "The context/category for the task"
        - name: list_context_tasks
          type: function
          python_path: examples.memory_task_management.task_tools.list_context_tasks
          description: "List tasks in a specific context"
          parameters:
            context:
              type: string
              description: "The context to list tasks from"

    - name: StatelessContextViewer
      type: agent
      llm_config:
        model: ${LLM_MODEL}
        api_key: ${LLM_API_KEY}
        base_url: ${LLM_BASE_URL}
      system_message: "You are a stateless context viewer that shows available contexts without maintaining conversation history."
      keep_history: false
      tools:
        - name: get_contexts
          type: function
          python_path: examples.memory_task_management.task_tools.get_contexts
          description: "Get all available contexts"


================================================
FILE: examples/memory_task_management/example_memory_management.py
================================================
import os, sys
from dotenv import load_dotenv

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))
from src.xronai.config import load_yaml_config, AgentFactory

# Load environment variables
load_dotenv()

# Load and process the YAML configuration
config = load_yaml_config('examples/memory_task_management/config.yaml')

# Create the agent structure from YAML
factory = AgentFactory()
context_manager = factory.create_from_config(config)


def chat_with_agents(query: str):
    response = context_manager.chat(query)
    print(f"Context Manager: {response}")


if __name__ == "__main__":
    print("Welcome to the Context-Aware Task Management System!")
    print("\nThis example demonstrates the difference between stateful and stateless agents:")
    print("- StatefulTaskManager: Remembers context from previous interactions")
    print("- StatelessContextViewer: Treats each interaction independently")
    print("\nAvailable agents:")
    context_manager.display_agent_graph()
    print("\nTry these example interactions:")
    print("1. 'Add a task about documentation to the dev context'")
    print("2. 'What tasks are in that context?'")
    print("3. 'Show me all contexts'")
    print("\nType 'exit' to quit.")

    while True:
        user_input = input("\nYou: ")
        if user_input.lower() == 'exit':
            break
        chat_with_agents(user_input)



================================================
FILE: examples/memory_task_management/task_tools.py
================================================
# Dictionary to store tasks per context
context_tasks = {}


def add_task_with_context(task: str, context: str) -> str:
    """
    Add a new task with context information.
    Args:
        task (str): The task to be added
        context (str): The context/category for the task
    Returns:
        str: Confirmation message
    """
    if context not in context_tasks:
        context_tasks[context] = []
    context_tasks[context].append(task)
    return f"Task added to {context}: {task}"


def list_context_tasks(context: str) -> str:
    """
    List all tasks in a specific context.
    Args:
        context (str): The context/category to list tasks from
    Returns:
        str: List of tasks in the context
    """
    if context not in context_tasks:
        return f"No tasks found in context: {context}"
    tasks = context_tasks[context]
    return "\n".join(f"{i+1}. {task}" for i, task in enumerate(tasks))


def get_contexts() -> str:
    """
    Get all available contexts.
    Returns:
        str: List of all contexts
    """
    if not context_tasks:
        return "No contexts available"
    return "\n".join(f"- {context} ({len(tasks)} tasks)" for context, tasks in context_tasks.items())



================================================
FILE: examples/supervisor_multi_mcp/README.md
================================================
# Supervisor Multi-MCP Example

This example shows how to use a `Supervisor` to manage agents powered by different MCP tool servers:
- A weather info agent (using MCP stdio transport)
- An addition/math agent (using MCP SSE transport)

## Folder Contents

- `add_server.py` &mdash; MCP SSE server providing an `add(a, b)` tool on port 8000.
- `weather_server.py` &mdash; MCP stdio server providing weather tools (`get_alerts`, `get_forecast`).
- `example_multi_mcp.py` &mdash; Main supervisor/agent orchestrator and chat frontend.
- `README.md` &mdash; This info.


## Setup and Usage

1. **Start the Addition SSE Server**

   In one terminal:
   ```
   python add_server.py
   ```

2. **(Nothing to run for Weather Stdio! The agent code will launch weather_server.py as needed.)**

3. **Start the Supervisor Chat**

   In another terminal:
   ```
   python example_multi_mcp.py
   ```

   - Ask: `What are the weather alerts for NY?`
   - Ask: `What is 2 plus 4?`
   - Mix weather and math prompts; the supervisor will route queries!

## Pro Tips

- The agent graph will be displayed before chat.
- No need to run `weather_server.py` manually; agent launches it via stdio as a subprocess.
- You can extend with more agents and tool servers, using either MCP transport.



================================================
FILE: examples/supervisor_multi_mcp/add_server.py
================================================
from mcp.server.fastmcp import FastMCP

mcp = FastMCP("Addition SSE Server")


@mcp.tool()
def add(a: int, b: int) -> int:
    """Add two numbers."""
    return a + b


if __name__ == "__main__":
    mcp.run(transport='sse')



================================================
FILE: examples/supervisor_multi_mcp/example_multi_mcp.py
================================================
import os, sys
from dotenv import load_dotenv

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))
from src.xronai.core import Agent, Supervisor

# Load environment variables
load_dotenv()

# LLM Configuration
llm_config = {
    'model': os.getenv('LLM_MODEL'),
    'api_key': os.getenv('LLM_API_KEY'),
    'base_url': os.getenv('LLM_BASE_URL')
}

# MCP servers configuration for each agent
weather_mcp = {"type": "stdio", "script_path": os.path.join(os.path.dirname(__file__), "weather_server.py")}
add_mcp = {
    "type": "sse",
    "url": "http://localhost:8000/sse",  # SSE server
}

weather_agent = Agent(name="WeatherAgent",
                      llm_config=llm_config,
                      system_message="You answer weather questions by using tools.",
                      mcp_servers=[weather_mcp],
                      use_tools=True,
                      keep_history=True)

add_agent = Agent(name="AdditionAgent",
                  llm_config=llm_config,
                  system_message="You answer addition and math calculation queries.",
                  mcp_servers=[add_mcp],
                  use_tools=True,
                  keep_history=True)

supervisor = Supervisor(name="MultiMCP_Supervisor",
                        llm_config=llm_config,
                        system_message="You delegate math and weather queries to your agents.")

supervisor.register_agent(weather_agent)
supervisor.register_agent(add_agent)

print("\n=== Agent Hierarchy ===")
supervisor.display_agent_graph()
print("\n=== Chat with your AI Team! ===")
supervisor.start_interactive_session()



================================================
FILE: examples/supervisor_multi_mcp/weather_server.py
================================================
from typing import Any
import httpx
from mcp.server.fastmcp import FastMCP

mcp = FastMCP("Weather Stdio Server")
NWS_API_BASE = "https://api.weather.gov"
USER_AGENT = "weather-app/1.0"


async def make_nws_request(url: str) -> dict[str, Any] | None:
    headers = {"User-Agent": USER_AGENT, "Accept": "application/geo+json"}
    async with httpx.AsyncClient() as client:
        try:
            resp = await client.get(url, headers=headers, timeout=30.0)
            resp.raise_for_status()
            return resp.json()
        except Exception:
            return None


def format_alert(f: dict) -> str:
    p = f["properties"]
    return (f"\nEvent: {p.get('event', 'Unknown')}"
            f"\nArea: {p.get('areaDesc', 'Unknown')}"
            f"\nSeverity: {p.get('severity', 'Unknown')}"
            f"\nDescription: {p.get('description', 'No description')}"
            f"\nInstructions: {p.get('instruction', 'No instructions')}\n")


@mcp.tool()
async def get_alerts(state: str) -> str:
    """Get weather alerts for a US state (e.g. CA, NY)."""
    url = f"{NWS_API_BASE}/alerts/active/area/{state.upper()}"
    data = await make_nws_request(url)
    if not data or "features" not in data:
        return "Unable to fetch alerts or no alerts found."
    if not data["features"]:
        return "No active alerts for this state."
    alerts = [format_alert(f) for f in data["features"]]
    return "\n---\n".join(alerts)


@mcp.tool()
async def get_forecast(latitude: float, longitude: float) -> str:
    """Get weather forecast for a location (lat, lon)."""
    points_url = f"{NWS_API_BASE}/points/{latitude},{longitude}"
    points_data = await make_nws_request(points_url)
    if (not points_data or "properties" not in points_data or "forecast" not in points_data["properties"]):
        return "Unable to fetch forecast data for this location."
    forecast_url = points_data["properties"]["forecast"]
    forecast_data = await make_nws_request(forecast_url)
    if (not forecast_data or "properties" not in forecast_data or "periods" not in forecast_data["properties"]):
        return "Unable to fetch detailed forecast."
    periods = forecast_data["properties"]["periods"]
    forecasts = []
    for period in periods[:5]:
        forecasts.append(f"\n{period['name']}:"
                         f"\nTemperature: {period['temperature']}°{period['temperatureUnit']}"
                         f"\nWind: {period['windSpeed']} {period['windDirection']}"
                         f"\nForecast: {period['detailedForecast']}\n")
    return "\n---\n".join(forecasts)


if __name__ == "__main__":
    mcp.run(transport='stdio')



================================================
FILE: examples/task_management_with_yaml/config.yaml
================================================
workflow_id: "task_management_workflow"  # Optional, will be auto-generated if not provided
supervisor:
  name: TaskManager
  type: supervisor
  is_assistant: false  # Main supervisor must be false
  llm_config:
    model: ${LLM_MODEL}
    api_key: ${LLM_API_KEY}
    base_url: ${LLM_BASE_URL}
  system_message: "You are the task management supervisor. Remember, the user can only see your message, so provide complete information from the agent's/supervisors message as well."
  children:
    - name: TaskCreator
      type: agent
      llm_config:
        model: ${LLM_MODEL}
        api_key: ${LLM_API_KEY}
        base_url: ${LLM_BASE_URL}
      system_message: "You are responsible for creating new tasks."
      keep_history: true  # Optional, defaults to true
      tools:
        - name: add_task
          type: function
          python_path: examples.task_management_with_yaml.task_tools.add_task
          description: "Add a new task to the list"
          parameters:
            task:
              type: string
              description: "The task to be added"

    - name: TaskOrganizer
      type: supervisor
      is_assistant: true  # Must be true for child supervisors
      llm_config:
        model: ${LLM_MODEL}
        api_key: ${LLM_API_KEY}
        base_url: ${LLM_BASE_URL}
      system_message: "You are a sub-supervisor managing task listing and prioritization. Remember, the user can only see your message, so provide complete information from the agent's message as well."
      children:
        - name: TaskLister
          type: agent
          llm_config:
            model: ${LLM_MODEL}
            api_key: ${LLM_API_KEY}
            base_url: ${LLM_BASE_URL}
          system_message: "You are responsible for listing tasks."
          keep_history: true
          tools:
            - name: list_tasks
              type: function
              python_path: examples.task_management_with_yaml.task_tools.list_tasks
              description: "List all current tasks"

        - name: PriorityCalculator
          type: agent
          llm_config:
            model: ${LLM_MODEL}
            api_key: ${LLM_API_KEY}
            base_url: ${LLM_BASE_URL}
          system_message: "You calculate the priority of tasks based on urgency and importance."
          keep_history: true
          tools:
            - name: calculate_priority
              type: function
              python_path: examples.task_management_with_yaml.task_tools.calculate_priority
              description: "Calculate the priority of a task"
              parameters:
                urgency:
                  type: integer
                  description: "The urgency of the task (1-10)"
                importance:
                  type: integer
                  description: "The importance of the task (1-10)"


================================================
FILE: examples/task_management_with_yaml/example_task_management.py
================================================
import os, sys
from dotenv import load_dotenv

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))
from src.xronai.config import load_yaml_config, AgentFactory

# Load environment variables
load_dotenv()

# Load and process the YAML configuration
config = load_yaml_config('examples/task_management_with_yaml/config.yaml')

# Create the agent structure from YAML
factory = AgentFactory()
task_manager = factory.create_from_config(config)


# Function to handle user input and agent responses
def chat_with_agents(query: str):
    response = task_manager.chat(query)
    print(f"Task Manager: {response}")


# Main interaction loop
if __name__ == "__main__":
    print("Welcome to the Task Management System!")
    print("You can interact with the following agents:")
    task_manager.display_agent_graph()
    print("Type 'exit' to quit.")

    while True:
        user_input = input("\nYou: ")
        if user_input.lower() == 'exit':
            break
        chat_with_agents(user_input)



================================================
FILE: examples/task_management_with_yaml/task_tools.py
================================================
tasks = ["Implement YAML config", "Write tests", "Update documentation"]


def add_task(task: str) -> str:
    """
    Add a new task to the task list.

    Args:
        task (str): The task to be added.

    Returns:
        str: Confirmation message.
    """
    tasks.append(task)
    return f"Task added: {task}"


def list_tasks() -> str:
    """
    List all current tasks.

    Returns:
        str: Numbered list of all tasks.
    """
    return "\n".join(f"{i+1}. {task}" for i, task in enumerate(tasks))


def calculate_priority(urgency: int, importance: int) -> int:
    """
    Calculate the priority of a task.

    Args:
        urgency (int): The urgency of the task (1-10).
        importance (int): The importance of the task (1-10).

    Returns:
        int: The calculated priority.
    """
    return urgency * importance



================================================
FILE: src/xronai/__init__.py
================================================
"""XronAI: The Python SDK for building powerful, agentic AI chatbots."""

from .core.supervisor import Supervisor
from .core.agents import Agent

__all__ = [
    "Supervisor",
    "Agent",
]

__version__ = "0.1.3"



================================================
FILE: src/xronai/config/__init__.py
================================================
from .yaml_config import load_yaml_config
from .config_validator import ConfigValidator, ConfigValidationError
from .agent_factory import AgentFactory

__all__ = ['load_yaml_config', 'ConfigValidator', 'ConfigValidationError', 'AgentFactory']



================================================
FILE: src/xronai/config/agent_factory.py
================================================
"""
AgentFactory module for creating hierarchical structures of Supervisors and Agents.

This module provides a factory class that constructs a complete hierarchy of
Supervisors and Agents based on a configuration dictionary, typically loaded
from a YAML file.
"""

import importlib, uuid
from typing import Dict, Any, List, Optional
from xronai.core import Supervisor, Agent
from .config_validator import ConfigValidator


class AgentFactory:
    """
    A factory class for creating hierarchical structures of Supervisors and Agents.

    This class provides static methods to create a complete hierarchy of
    Supervisors and Agents based on a configuration dictionary. It handles
    the creation of tools for Agents and ensures proper nesting of
    Supervisors and Agents.
    """

    @staticmethod
    async def create_from_config(config: Dict[str, Any]) -> Supervisor:
        """
        Create a Supervisor with its entire hierarchy from a configuration dictionary.

        Args:
            config (Dict[str, Any]): The configuration dictionary containing
                the entire hierarchy structure.

        Returns:
            Supervisor: The root Supervisor of the created hierarchy.

        Raises:
            ConfigValidationError: If the configuration is invalid.
        """
        ConfigValidator.validate(config)
        # Generate workflow_id if not provided
        workflow_id = config.get('workflow_id', str(uuid.uuid4()))
        return await AgentFactory._create_supervisor(config['supervisor'], is_root=True, workflow_id=workflow_id)

    @staticmethod
    async def _create_supervisor(supervisor_config: Dict[str, Any],
                                 is_root: bool = False,
                                 workflow_id: Optional[str] = None) -> Supervisor:
        """
        Create a Supervisor instance and its children from a configuration dictionary.

        Args:
            supervisor_config (Dict[str, Any]): The configuration for this Supervisor.
            is_root (bool): Whether this Supervisor is the root of the hierarchy.
            workflow_id (Optional[str]): ID of the workflow (only for root supervisor).

        Returns:
            Supervisor: The created Supervisor instance with all its children.
        """
        supervisor = Supervisor(name=supervisor_config['name'],
                                llm_config=supervisor_config['llm_config'],
                                system_message=supervisor_config['system_message'],
                                workflow_id=workflow_id if is_root else None,
                                is_assistant=supervisor_config.get('is_assistant', False))

        for child_config in supervisor_config.get('children', []):
            if child_config['type'] == 'supervisor':
                child = await AgentFactory._create_supervisor(child_config, is_root=False, workflow_id=None)
            else:  # agent
                child = await AgentFactory._create_agent(child_config)
            supervisor.register_agent(child)

        return supervisor

    @staticmethod
    async def _create_agent(agent_config: Dict[str, Any]) -> Agent:
        """
        Create an Agent instance from a configuration dictionary.

        Args:
            agent_config (Dict[str, Any]): The configuration for this Agent.

        Returns:
            Agent: The created Agent instance with its tools.
        """
        tools = AgentFactory._create_tools(agent_config.get('tools', []))

        agent_params = {
            'name': agent_config['name'],
            'llm_config': agent_config['llm_config'],
            'system_message': agent_config['system_message'],
            'tools': tools,
            'use_tools': agent_config.get('use_tools', bool(tools)),
            'keep_history': agent_config.get('keep_history', True),
            'output_schema': agent_config.get('output_schema'),
            'strict': agent_config.get('strict', False),
            'mcp_servers': agent_config.get('mcp_servers', [])
        }

        agent = Agent(**agent_params)
        await agent._load_mcp_tools()
        return agent

    @staticmethod
    def _create_tools(tools_config: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Create a list of tool configurations from the provided tool configs.

        Args:
            tools_config (List[Dict[str, Any]]): List of tool configurations.

        Returns:
            List[Dict[str, Any]]: List of created tool configurations.
        """
        tools = []
        for tool_config in tools_config:
            tool_function = AgentFactory._import_function(tool_config['python_path'])
            metadata = {
                "type": "function",
                "function": {
                    "name": tool_config['name'],
                    "description": tool_config.get('description', ''),
                    "parameters": {
                        "type": "object",
                        "properties": tool_config.get('parameters', {}),
                        "required": list(tool_config.get('parameters', {}).keys())
                    }
                }
            }
            tools.append({"tool": tool_function, "metadata": metadata})
        return tools

    @staticmethod
    def _import_function(python_path: str):
        """
        Import a function from a given Python path.

        Args:
            python_path (str): The full path to the function, including module.

        Returns:
            Callable: The imported function.

        Raises:
            ImportError: If the module or function cannot be imported.
            AttributeError: If the specified function is not found in the module.
        """
        module_name, function_name = python_path.rsplit('.', 1)
        module = importlib.import_module(module_name)
        return getattr(module, function_name)



================================================
FILE: src/xronai/config/config_validator.py
================================================
"""
ConfigValidator module for validating the configuration of Supervisors and Agents.

This module provides a ConfigValidator class that performs thorough validation
of the configuration dictionary used to create hierarchical structures of
Supervisors and Agents.
"""

from typing import Dict, Any, List


class ConfigValidationError(Exception):
    """
    Custom exception for configuration validation errors.

    This exception is raised when the configuration fails to meet
    the required structure or contains invalid data.
    """
    pass


class ConfigValidator:
    """
    A validator class for checking the structure and content of configuration dictionaries.

    This class provides static methods to validate the entire configuration
    hierarchy, including Supervisors, Agents, LLM configurations, and tools.
    """

    @staticmethod
    def validate(config: Dict[str, Any]) -> None:
        """
        Validate the entire configuration dictionary.

        Args:
            config (Dict[str, Any]): The configuration dictionary to validate.

        Raises:
            ConfigValidationError: If the configuration is invalid.
        """
        ConfigValidator._validate_supervisor(config.get('supervisor', {}), is_root=True)

    @staticmethod
    def _validate_supervisor(supervisor: Dict[str, Any], is_root: bool = False) -> None:
        """
        Validate a supervisor configuration.

        Args:
            supervisor (Dict[str, Any]): The supervisor configuration to validate.
            is_root (bool): Whether this supervisor is the root of the hierarchy.

        Raises:
            ConfigValidationError: If the supervisor configuration is invalid.
        """
        required_fields = ['name', 'type', 'llm_config', 'system_message']
        if is_root:
            required_fields.append('children')

        for field in required_fields:
            if field not in supervisor:
                raise ConfigValidationError(f"Missing required field '{field}' in supervisor configuration")

        if supervisor['type'] != 'supervisor':
            raise ConfigValidationError(f"Invalid type for supervisor: {supervisor['type']}")

        if 'is_assistant' in supervisor and not isinstance(supervisor['is_assistant'], bool):
            raise ConfigValidationError("'is_assistant' must be a boolean value")

        if is_root and supervisor.get('is_assistant', False):
            raise ConfigValidationError("Root supervisor cannot be an assistant supervisor")

        ConfigValidator._validate_llm_config(supervisor['llm_config'])

        for child in supervisor.get('children', []):
            if child['type'] == 'supervisor':
                if not child.get('is_assistant', False):
                    raise ConfigValidationError("Child supervisors must be assistant supervisors")
                ConfigValidator._validate_supervisor(child)
            elif child['type'] == 'agent':
                ConfigValidator._validate_agent(child)
            else:
                raise ConfigValidationError(f"Invalid type for child: {child['type']}")

    @staticmethod
    def _validate_agent(agent: Dict[str, Any]) -> None:
        """
        Validate an agent configuration.

        Args:
            agent (Dict[str, Any]): The agent configuration to validate.

        Raises:
            ConfigValidationError: If the agent configuration is invalid.
        """
        required_fields = ['name', 'type', 'llm_config', 'system_message']
        for field in required_fields:
            if field not in agent:
                raise ConfigValidationError(f"Missing required field '{field}' in agent configuration")

        bool_fields = ['keep_history', 'use_tools', 'strict']
        for field in bool_fields:
            if field in agent and not isinstance(agent[field], bool):
                raise ConfigValidationError(f"'{field}' must be a boolean value")

        if 'output_schema' in agent:
            if not isinstance(agent['output_schema'], dict):
                raise ConfigValidationError("output_schema must be a dictionary")
            if 'type' not in agent['output_schema']:
                raise ConfigValidationError("output_schema must have 'type' field")

        if 'mcp_servers' in agent:
            if not isinstance(agent['mcp_servers'], list):
                raise ConfigValidationError("mcp_servers must be a list")
            for server in agent['mcp_servers']:
                if 'type' not in server:
                    raise ConfigValidationError("Each MCP server must have 'type' field")
                if server['type'] not in ['sse', 'stdio']:
                    raise ConfigValidationError("MCP server type must be 'sse' or 'stdio'")
                if server['type'] == 'sse' and 'url' not in server:
                    raise ConfigValidationError("SSE MCP server must have 'url' field")
                if server['type'] == 'stdio' and 'script_path' not in server:
                    raise ConfigValidationError("stdio MCP server must have 'script_path' field")

        ConfigValidator._validate_llm_config(agent['llm_config'])
        ConfigValidator._validate_tools(agent.get('tools', []))

    @staticmethod
    def _validate_llm_config(llm_config: Dict[str, Any]) -> None:
        """
        Validate the LLM (Language Model) configuration.

        Args:
            llm_config (Dict[str, Any]): The LLM configuration to validate.

        Raises:
            ConfigValidationError: If the LLM configuration is invalid.
        """
        required_fields = ['model', 'api_key', 'base_url']
        for field in required_fields:
            if field not in llm_config:
                raise ConfigValidationError(f"Missing required field '{field}' in llm_config")

    @staticmethod
    def _validate_tools(tools: List[Dict[str, Any]]) -> None:
        """
        Validate the list of tools in an agent's configuration.

        Args:
            tools (List[Dict[str, Any]]): The list of tool configurations to validate.

        Raises:
            ConfigValidationError: If any tool configuration is invalid.
        """
        for tool in tools:
            required_fields = ['name', 'type', 'python_path']
            for field in required_fields:
                if field not in tool:
                    raise ConfigValidationError(f"Missing required field '{field}' in tool configuration")

            if tool['type'] != 'function':
                raise ConfigValidationError(f"Invalid tool type: {tool['type']}")



================================================
FILE: src/xronai/config/yaml_config.py
================================================
"""
YAML configuration loader module.

This module provides functions for loading YAML configuration files
and expanding environment variables within the configuration.
"""

import os
import yaml
from typing import Dict, Any


def load_yaml_config(file_path: str) -> Dict[str, Any]:
    """
    Load a YAML configuration file and expand its environment variables.

    This function reads a YAML file, parses its contents, and then
    expands any environment variables found within the configuration.

    Args:
        file_path (str): The path to the YAML configuration file.

    Returns:
        Dict[str, Any]: The loaded and processed configuration as a dictionary.

    Raises:
        FileNotFoundError: If the specified file_path does not exist.
        yaml.YAMLError: If there's an error parsing the YAML file.
        IOError: If there's an error reading the file.
    """
    try:
        with open(file_path, 'r') as file:
            config = yaml.safe_load(file)
        return expand_env_vars(config)
    except FileNotFoundError:
        raise FileNotFoundError(f"Configuration file not found: {file_path}")
    except yaml.YAMLError as e:
        raise yaml.YAMLError(f"Error parsing YAML file: {e}")
    except IOError as e:
        raise IOError(f"Error reading configuration file: {e}")


def expand_env_vars(config: Any) -> Any:
    """
    Recursively expand environment variables in a configuration.

    This function traverses through the configuration data structure
    (which can be a nested dictionary, list, or a string) and expands
    any environment variables it encounters.

    Args:
        config (Any): The configuration item to process. Can be a dict, list, str, or any other type.

    Returns:
        Any: The processed configuration item with expanded environment variables.

    Note:
        - Environment variables should be in the format ${VAR_NAME} or $VAR_NAME.
        - If an environment variable is not set, it will be left unexpanded.
        - Non-string types are returned as-is.
    """
    if isinstance(config, dict):
        return {k: expand_env_vars(v) for k, v in config.items()}
    elif isinstance(config, list):
        return [expand_env_vars(i) for i in config]
    elif isinstance(config, str):
        return os.path.expandvars(config)
    return config



================================================
FILE: src/xronai/core/__init__.py
================================================
from .ai import AI
from .agents import Agent
from .supervisor import Supervisor

__all__ = ['AI', 'Agent', 'Supervisor']



================================================
FILE: src/xronai/core/agents.py
================================================
"""
Agent module for handling specialized AI interactions.

This module provides an Agent class that extends the base AI functionality
with additional features like tool usage and chat history management.
"""

import json, asyncio, uuid
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Any, Callable
from openai.types.chat import ChatCompletionMessage
from xronai.core.ai import AI
from xronai.history import HistoryManager, EntityType
from xronai.utils import Debugger
from mcp import ClientSession, StdioServerParameters
from mcp.client.sse import sse_client
from mcp.client.stdio import stdio_client


class Agent(AI):
    """
    An Agent class that extends the base AI functionality.

    This class handles specialized interactions, including the use of tools
    and management of chat history. It can operate independently or as part
    of a supervised workflow.
    """

    def __init__(self,
                 name: str,
                 llm_config: Dict[str, str],
                 workflow_id: Optional[str] = None,
                 tools: Optional[List[Dict[str, Any]]] = None,
                 system_message: Optional[str] = None,
                 use_tools: bool = False,
                 keep_history: bool = True,
                 mcp_servers: Optional[List[Dict[str, Any]]] = None,
                 output_schema: Optional[Dict[str, Any]] = None,
                 strict: bool = False):
        """
        Initialize the Agent instance.

        Args:
            name (str): The name of the agent.
            llm_config (Dict[str, str]): Configuration for the language model.
            workflow_id (Optional[str]): ID of the workflow. If provided, the agent will
                                       initialize its own persistent history. If not, one
                                       will be assigned by a Supervisor upon registration.
            tools (Optional[List[Dict[str, Any]]]): List of tools available to the agent.
            system_message (Optional[str]): The initial system message for the agent.
            use_tools (bool): Whether to use tools in interactions.
            keep_history (bool): Whether to maintain chat history between interactions.
            mcp_servers : Optional[List[Dict[str, Any]]], default None
                List of dicts, where each defines an MCP server/proxy:
                - For remote/SSE: {'type': 'sse', 'url': ..., 'auth_token': ...}
                - For local/stdio: {'type': 'stdio', 'script_path': 'server.py'}
                All discovered tools are available as functions to the agent.
            output_schema (Optional[Dict[str, Any]]): Schema for agent's output format.
            strict (bool): If True, always enforce output schema.

        Raises:
            ValueError: If the name is empty.
        """
        super().__init__(llm_config=llm_config)

        if not name:
            raise ValueError("Agent name cannot be empty")

        self.name = name
        self.workflow_id = workflow_id
        self.use_tools = use_tools
        self.tools = tools or []
        self.system_message = system_message
        self.keep_history = keep_history
        self.history_manager = None
        self.debugger = Debugger(name=self.name, workflow_id=self.workflow_id)
        self.debugger.start_session()
        self.chat_history: List[Dict[str, str]] = []
        self.mcp_servers = mcp_servers or []
        self._mcp_tool_names = set()
        self.output_schema = output_schema
        self.strict = strict

        if system_message:
            self.set_system_message(system_message)

        # --- NEW LOGIC: Make Agent self-sufficient for history ---
        if self.workflow_id and self.keep_history:
            self._initialize_workflow()
        # --------------------------------------------------------

        try:
            asyncio.get_running_loop()
        except RuntimeError:
            if self.mcp_servers:
                asyncio.run(self._load_mcp_tools())

    def _initialize_workflow(self):
        """
        Initializes the workflow directory and history manager for a standalone agent.
        """
        if not self.workflow_id:
            self.workflow_id = str(uuid.uuid4())

        self.debugger.update_workflow_id(self.workflow_id)

        workflow_path = Path("xronai_logs") / self.workflow_id
        workflow_path.mkdir(parents=True, exist_ok=True)

        history_file = workflow_path / "history.jsonl"
        if not history_file.exists():
            history_file.touch()

        self.history_manager = HistoryManager(self.workflow_id)

        # Save system message and load previous history
        self._initialize_chat_history()

        # Load chat history from disk
        self.chat_history = self.history_manager.load_chat_history(self.name)
        if len(self.chat_history) > 1:  # More than just the system message
            self.debugger.log(f"Previous history loaded for standalone agent {self.name}.")

    def _initialize_chat_history(self):
        """Saves the system message to history if it doesn't already exist."""
        if self.system_message and self.history_manager:
            if not self.history_manager.has_system_message(self.name):
                self.history_manager.append_message(message={
                    "role": "system",
                    "content": self.system_message
                },
                                                    sender_type=EntityType.AGENT,
                                                    sender_name=self.name)

    def _emit_event(self, on_event: Optional[Callable], event_type: str, data: Dict[str, Any]):
        """Safely emits an event if the callback is provided."""
        if on_event:
            payload = {
                "id": f"evt_{uuid.uuid4()}",
                "timestamp": datetime.utcnow().isoformat() + "Z",
                "type": event_type,
                "data": data,
            }
            on_event(payload)

    def set_workflow_id(self, workflow_id: str) -> None:
        """
        Set the workflow ID and initialize the history manager.
        This method is called by the Supervisor when registering the agent.

        Args:
            workflow_id (str): The workflow ID to set.
        """
        # --- MODIFIED LOGIC: Prevent re-initialization ---
        if self.workflow_id:
            # If the agent already initialized its own workflow, just update the debugger
            self.debugger.update_workflow_id(workflow_id)
            return
        # -------------------------------------------------

        self.workflow_id = workflow_id
        self.debugger.update_workflow_id(workflow_id)
        self.history_manager = HistoryManager(workflow_id)
        self._initialize_chat_history()

    def set_system_message(self, message: str) -> None:
        """
        Set the system message for the agent, including output schema if specified.

        Args:
            message (str): The system message to set.
        """
        if self.output_schema:
            schema_instruction = ("\n\nYOU MUST ALWAYS RESPOND IN THE FOLLOWING FORMAT:\n"
                                  f"{json.dumps(self.output_schema, indent=2)}\n"
                                  "Your entire response must be valid JSON matching this schema.\n")
            message = message + schema_instruction

        self.system_message = message
        self._reset_chat_history()

    def _validate_and_format_response(self, response: str) -> str:
        """
        Validate response against schema and reformat if needed.
        
        Args:
            response (str): Raw response from LLM
            
        Returns:
            str: Validated/formatted response
        """
        if not self.output_schema:
            return response

        try:
            parsed = json.loads(response)
            return json.dumps(parsed)
        except json.JSONDecodeError:
            if not self.strict:
                return response

            format_prompt = (f"Given this response:\n'''\n{response}\n'''\n"
                             f"Reformat it to match this schema:\n{json.dumps(self.output_schema, indent=2)}\n"
                             "Return ONLY the formatted JSON, nothing else.")

            formatted = self.generate_response(messages=[{
                "role": "user",
                "content": format_prompt
            }]).choices[0].message.content

            try:
                return json.dumps(json.loads(formatted))
            except json.JSONDecodeError:
                self.debugger.log("Schema enforcement failed", level="error")
                return response

    def chat(self, query: str, sender_name: Optional[str] = None, on_event: Optional[Callable] = None) -> str:
        """
        Process a chat interaction with the agent.

        Args:
            query (str): The query to process.
            sender_name (Optional[str]): Name of the entity sending the query.
                                       If None, this agent is treated as the top-level entry point.
            on_event (Optional[Callable]): A callback function to stream events to.

        Returns:
            str: The agent's response to the query.

        Raises:
            RuntimeError: If there's an error processing the query or using tools.
        """
        self.debugger.log(f"Query received from {sender_name or 'direct'}: {query}")

        is_entry_point = sender_name is None
        if is_entry_point:
            self._emit_event(on_event, "WORKFLOW_START", {"user_query": query})

        if not self.keep_history:
            self._reset_chat_history()

        user_msg = {'role': 'user', 'content': query}
        self.chat_history.append(user_msg)

        query_msg_id = None
        if self.history_manager:
            sender_type = (EntityType.MAIN_SUPERVISOR if sender_name else EntityType.USER)
            query_msg_id = self.history_manager.append_message(message=user_msg,
                                                               sender_type=sender_type,
                                                               sender_name=sender_name or "user")

        while True:
            try:
                response = self.generate_response(self.chat_history,
                                                  tools=[tool['metadata'] for tool in self.tools],
                                                  use_tools=self.use_tools).choices[0]

                if not response.finish_reason == "tool_calls":
                    user_query_answer = response.message.content
                    user_query_answer = self._validate_and_format_response(user_query_answer)
                    self.debugger.log(f"{self.name} response: {user_query_answer}")

                    response_msg = {"role": "assistant", "content": user_query_answer}
                    self.chat_history.append(response_msg)

                    if self.history_manager:
                        self.history_manager.append_message(message=response_msg,
                                                            sender_type=EntityType.AGENT,
                                                            sender_name=self.name,
                                                            parent_id=query_msg_id)

                    if is_entry_point:
                        self._emit_event(on_event, "FINAL_RESPONSE", {
                            "source": {
                                "name": self.name,
                                "type": "AGENT"
                            },
                            "content": user_query_answer
                        })
                        self._emit_event(on_event, "WORKFLOW_END", {})

                    return user_query_answer

                tool_call = response.message.tool_calls[0]
                tool_msg = {
                    "role":
                        "assistant",
                    "content":
                        None,
                    "tool_calls": [{
                        'id': tool_call.id,
                        'type': 'function',
                        'function': {
                            'name': tool_call.function.name,
                            'arguments': tool_call.function.arguments
                        }
                    }]
                }
                self.chat_history.append(tool_msg)

                tool_msg_id = None
                if self.history_manager:
                    tool_msg_id = self.history_manager.append_message(message=tool_msg,
                                                                      sender_type=EntityType.AGENT,
                                                                      sender_name=self.name,
                                                                      parent_id=query_msg_id,
                                                                      tool_call_id=tool_call.id)

                self._process_tool_call(response.message, tool_msg_id, on_event=on_event)

            except Exception as e:
                error_msg = f"Error in chat processing: {str(e)}"
                self._emit_event(on_event, "ERROR", {
                    "source": {
                        "name": self.name,
                        "type": "AGENT"
                    },
                    "error_message": error_msg
                })
                if is_entry_point:
                    self._emit_event(on_event, "WORKFLOW_END", {})
                self.debugger.log(error_msg)
                raise RuntimeError(error_msg)

    def _process_tool_call(self,
                           message: ChatCompletionMessage,
                           parent_msg_id: Optional[str] = None,
                           on_event: Optional[Callable] = None) -> None:
        """
        Process a tool call from the chat response.

        Args:
            message (ChatCompletionMessage): The message containing the tool call.
            parent_msg_id (Optional[str]): ID of the parent message in history.
            on_event (Optional[Callable]): The event callback function.

        Raises:
            ValueError: If the specified tool is not found or if there's an error in processing arguments.
        """
        if not hasattr(message, 'tool_calls') or not message.tool_calls:
            raise ValueError("Message does not contain tool calls")

        function_call = message.tool_calls[0]
        target_tool_name = function_call.function.name

        self.debugger.log(f"Initiating tool call: {target_tool_name}")

        try:
            tool_arguments = json.loads(function_call.function.arguments)
            self.debugger.log(f"Tool arguments: {json.dumps(tool_arguments, indent=2)}")
        except json.JSONDecodeError:
            error_msg = f"Invalid JSON in function arguments: {function_call.function.arguments}"
            self.debugger.log(error_msg, level="error")
            raise ValueError(error_msg)

        self._emit_event(
            on_event, "AGENT_TOOL_CALL", {
                "source": {
                    "name": self.name,
                    "type": "AGENT"
                },
                "tool_name": target_tool_name,
                "tool_call_id": function_call.id,
                "arguments": tool_arguments
            })

        target_tool = next((tool for tool in self.tools if tool['metadata']['function']['name'] == target_tool_name),
                           None)

        if not target_tool:
            error_msg = f"Tool '{target_tool_name}' not found"
            self.debugger.log(error_msg, level="error")
            raise ValueError(error_msg)

        tool_function = target_tool['tool']

        try:
            if hasattr(tool_function, '__kwdefaults__'):
                tool_feedback = tool_function(**tool_arguments)
            else:
                tool_feedback = tool_function(tool_arguments)

            self.debugger.log(f"Tool execution successful")
            self.debugger.log(f"Tool response: {str(tool_feedback)}")

            self._emit_event(
                on_event, "AGENT_TOOL_RESPONSE", {
                    "source": {
                        "name": target_tool_name,
                        "type": "TOOL"
                    },
                    "tool_call_id": function_call.id,
                    "result": str(tool_feedback)
                })

            tool_response_msg = {"role": "tool", "content": str(tool_feedback), "tool_call_id": function_call.id}
            self.chat_history.append(tool_response_msg)

            if self.history_manager:
                self.history_manager.append_message(message=tool_response_msg,
                                                    sender_type=EntityType.TOOL,
                                                    sender_name=target_tool_name,
                                                    parent_id=parent_msg_id,
                                                    tool_call_id=function_call.id)

        except Exception as e:
            error_msg = f"Tool execution failed: {str(e)}"
            self._emit_event(on_event, "ERROR", {
                "source": {
                    "name": target_tool_name,
                    "type": "TOOL"
                },
                "error_message": error_msg
            })
            self.debugger.log(error_msg, level="error")
            raise RuntimeError(error_msg) from e

    async def _load_mcp_tools(self):
        """
        Discover and register tools from all MCP servers configured in self.mcp_servers.

        This method connects to each specified MCP server using the configured transport
        (either "sse" or "stdio"), retrieves the available tools, converts their schemas
        to OpenAI-compatible format, and registers proxy functions for each tool. It
        removes any previously loaded MCP tools before loading new ones.

        Raises:
            ValueError: If an unknown transport type is encountered in the MCP server config.
            Exception: For any network, process, or protocol-level error during tool discovery.
        """
        self._remove_all_mcp_tools()
        self._mcp_tool_names = set()
        for server in self.mcp_servers:
            ttype = server.get("type", "sse")  # default to sse
            try:
                if ttype == "sse":
                    url = server["url"]
                    auth_token = server.get("auth_token")
                    endpoint = url  # Use the user-supplied URL exactly as written
                    headers = {"Authorization": f"Bearer {auth_token}"} if auth_token else {}
                    async with sse_client(endpoint, headers=headers) as streams:
                        async with ClientSession(*streams) as session:
                            await session.initialize()
                            ntools_resp = await session.list_tools()
                            ntools = ntools_resp.tools
                            for tool in ntools:
                                openai_tool_meta = self._convert_mcp_tool_to_openai(tool)
                                tname = openai_tool_meta["function"]["name"]
                                proxy = self._build_mcp_tool_proxy(transport_type="sse",
                                                                   conf={
                                                                       "url": url,
                                                                       "auth_token": auth_token
                                                                   },
                                                                   tool_name=tname)
                                tool_dict = {"tool": proxy, "metadata": openai_tool_meta, "_mcp_tool": True}
                                self.tools.append(tool_dict)
                                self._mcp_tool_names.add(tname)
                elif ttype == "stdio":
                    script_path = server["script_path"]
                    server_params = StdioServerParameters(command="python", args=[script_path], env=None)
                    async with stdio_client(server_params) as (stdio, write):
                        async with ClientSession(stdio, write) as session:
                            await session.initialize()
                            ntools_resp = await session.list_tools()
                            ntools = ntools_resp.tools
                            for tool in ntools:
                                openai_tool_meta = self._convert_mcp_tool_to_openai(tool)
                                tname = openai_tool_meta["function"]["name"]
                                proxy = self._build_mcp_tool_proxy(transport_type="stdio",
                                                                   conf={"script_path": script_path},
                                                                   tool_name=tname)
                                tool_dict = {"tool": proxy, "metadata": openai_tool_meta, "_mcp_tool": True}
                                self.tools.append(tool_dict)
                                self._mcp_tool_names.add(tname)
                else:
                    raise ValueError(f"[MCP] Unknown transport type: {ttype}")
            except Exception as e:
                print(f"[MCP] Error loading tools from {server}: {e}")
        self.tools_metadata = [tool['metadata'] for tool in self.tools]

    def _convert_mcp_tool_to_openai(self, tool) -> Dict[str, Any]:
        """
        Convert an MCP tool object to an OpenAI-compatible function/tool schema.

        This method translates the MCP tool's name, description, and input schema
        into the OpenAI function calling format for inclusion in the agent's tool list.

        Args:
            tool: The MCP tool object as returned by the MCP server.

        Returns:
            Dict[str, Any]: The tool schema in OpenAI format, ready for tool calling.

        Note:
            - All input parameters will be set as required for compatibility with OpenAI.
        """
        openai_tool = {
            "type": "function",
            "function": {
                "name": tool.name,
                "description": getattr(tool, 'description', '') or "MCP tool.",
                "parameters": {
                    "type": "object",
                    "properties": {},
                    "required": []
                }
            }
        }
        if hasattr(tool, 'inputSchema') and tool.inputSchema:
            schema = tool.inputSchema
            property_names = []
            properties = schema.get("properties", {})
            for prop_name, prop_details in properties.items():
                prop_copy = {k: v for k, v in prop_details.items() if k != 'default'}
                openai_tool["function"]["parameters"]["properties"][prop_name] = prop_copy
                property_names.append(prop_name)
            openai_tool["function"]["parameters"]["required"] = property_names
        return openai_tool

    def _build_mcp_tool_proxy(self, transport_type, conf, tool_name):
        """
        Create a synchronous Python proxy function for invoking an MCP tool.

        Depending on the transport type ("sse" or "stdio"), this factory builds a proxy
        function that accepts tool arguments as keyword arguments, then manages the
        necessary asynchronous communication to invoke the MCP tool and retrieve the result.

        Args:
            transport_type (str): The MCP transport type ("sse" or "stdio").
            conf (dict): Connection configuration dictionary (e.g., URL or script_path).
            tool_name (str): Name of the tool to invoke on the MCP server.

        Returns:
            Callable: A Python function that accepts keyword arguments and returns the tool's result.

        Raises:
            Exception: If calling the MCP tool fails for transport or invocation reasons.
        """

        def proxy(**kwargs):

            async def _call_sse():
                url = conf["url"]
                auth_token = conf.get("auth_token")
                endpoint = url
                headers = {"Authorization": f"Bearer {auth_token}"} if auth_token else {}
                async with sse_client(endpoint, headers=headers) as streams:
                    async with ClientSession(*streams) as session:
                        await session.initialize()
                        result = await session.call_tool(tool_name, arguments=kwargs)
                        if hasattr(result, "content") and result.content:
                            return result.content[0].text
                        return str(result)

            async def _call_stdio():
                script_path = conf["script_path"]
                server_params = StdioServerParameters(command="python", args=[script_path], env=None)
                async with stdio_client(server_params) as (stdio, write):
                    async with ClientSession(stdio, write) as session:
                        await session.initialize()
                        result = await session.call_tool(tool_name, arguments=kwargs)
                        if hasattr(result, "content") and result.content:
                            return result.content[0].text
                        return str(result)

            try:
                if transport_type == "sse":
                    try:
                        loop = asyncio.get_running_loop()
                        return loop.run_until_complete(_call_sse())
                    except RuntimeError:
                        return asyncio.run(_call_sse())
                elif transport_type == "stdio":
                    try:
                        loop = asyncio.get_running_loop()
                        return loop.run_until_complete(_call_stdio())
                    except RuntimeError:
                        return asyncio.run(_call_stdio())
                else:
                    raise ValueError(f"Unknown MCP transport {transport_type}")
            except Exception as e:
                return f"[MCP] Tool '{tool_name}' call failed: {e}"

        return proxy

    def _remove_all_mcp_tools(self):
        """
        Removes all tools loaded from MCP servers from self.tools.
        """
        self.tools = [t for t in self.tools if not t.get('_mcp_tool', False)]
        self._mcp_tool_names = set()

    def get_chat_history(self) -> List[Dict[str, str]]:
        """
        Get the current chat history.

        Returns:
            List[Dict[str, str]]: The current chat history.
        """
        return self.chat_history

    async def update_mcp_tools(self):
        """
        Refresh the agent's tools by re-discovering available tools from all MCP servers.

        This method removes all previously registered MCP tools, re-connects to all configured
        MCP servers, and loads the updated tool lists into the agent. Call this method if you
        add, remove, or update tools on any MCP server during runtime.

        Raises:
            Exception: For any underlying error in the discovery or registration process.
        """
        await self._load_mcp_tools()

    def _reset_chat_history(self) -> None:
        """Reset chat history to initial state (system message only)."""
        self.chat_history = []
        if self.system_message:
            system_msg = {"role": "system", "content": self.system_message}
            self.chat_history = [system_msg]

            if self.history_manager:
                self.history_manager.append_message(message=system_msg,
                                                    sender_type=EntityType.AGENT,
                                                    sender_name=self.name)

    def __str__(self) -> str:
        """Return a string representation of the Agent instance."""
        return f"Agent(name={self.name}, use_tools={self.use_tools})"

    def __repr__(self) -> str:
        """Return a detailed string representation of the Agent instance."""
        return (f"Agent(name={self.name}, llm_config={self.llm_config}, "
                f"use_tools={self.use_tools}, tool_count={len(self.tools)})")



================================================
FILE: src/xronai/core/ai.py
================================================
"""
AI module for handling interactions with OpenAI's API.

This module provides a base AI class for generating responses using OpenAI's chat completions.
"""

import openai
from typing import List, Dict, Any, Optional
from openai.types.chat import ChatCompletion


class AI:
    """
    A base class for AI interactions using OpenAI's API.

    This class handles the configuration and execution of chat completions,
    including optional function calling with tools.
    """

    def __init__(self, llm_config: Dict[str, str]):
        """
        Initialize the AI instance.

        Args:
            llm_config (Dict[str, str]): Configuration for the language model.
                Must contain 'api_key' and 'model'. May optionally include 'base_url' and 'temperature'.

        Raises:
            ValueError: If required configuration keys are missing or if tools are enabled but not provided.
        """
        if not all(key in llm_config for key in ['api_key', 'model']):
            raise ValueError("llm_config must contain 'api_key' and 'model'")

        self.llm_config = llm_config
        self.client = openai.OpenAI(base_url=llm_config.get('base_url', 'https://api.openai.com/v1'),
                                    api_key=llm_config['api_key'])

    def generate_response(self,
                          messages: List[Dict[str, str]],
                          tools: Optional[List[Dict[str, Any]]] = None,
                          use_tools: bool = False) -> ChatCompletion:
        """
        Execute a chat completion.

        Args:
            messages (List[Dict[str, str]]): List of conversation messages.
            tools (Optional[List[Dict[str, Any]]]): List of tools for function calling.
            use_tools (bool): Whether to use function calling with tools.

        Returns:
            ChatCompletion: The response from the OpenAI API.

        Raises:
            openai.OpenAIError: If there's an error in the API call.
            ValueError: If tools are requested but not provided.
        """
        if use_tools and not tools:
            raise ValueError("Tools must be provided when use_tools is True")

        try:
            params = self.llm_config.copy()

            params.pop('api_key', None)
            params.pop('base_url', None)

            params['messages'] = messages

            if use_tools:
                params['tools'] = tools
                params['tool_choice'] = 'auto'

            return self.client.chat.completions.create(**params)

        except openai.OpenAIError as e:
            raise openai.OpenAIError(f"Chat completion failed: {str(e)}")

    def __str__(self) -> str:
        """Return a string representation of the AI instance."""
        return f"AI(model={self.llm_config['model']})"

    def __repr__(self) -> str:
        """Return a detailed string representation of the AI instance."""
        return f"AI(llm_config={self.llm_config})"



================================================
FILE: src/xronai/core/supervisor.py
================================================
"""
Supervisor module for managing multiple specialized AI agents.

This module provides a Supervisor class that coordinates interactions between
users and multiple specialized AI agents.
"""

import json, uuid
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional, Union, Callable
from openai.types.chat import ChatCompletionMessage
from xronai.core import AI
from xronai.core import Agent
from xronai.history import HistoryManager, EntityType
from xronai.utils import Debugger


class Supervisor(AI):
    """
    A Supervisor class that manages multiple specialized AI agents.

    This class handles user queries, delegates tasks to appropriate agents,
    and coordinates complex multi-step processes.
    
    The Supervisor can operate in two modes:
    1. Main Supervisor: Creates and manages the workflow directory
    2. Assistant Supervisor: Works within an existing workflow
    """

    def __init__(self,
                 name: str,
                 llm_config: Dict[str, str],
                 workflow_id: Optional[str] = None,
                 is_assistant: bool = False,
                 system_message: Optional[str] = None,
                 use_agents: bool = True):
        """
        Initialize the Supervisor instance.

        Args:
            name (str): The name of the supervisor.
            llm_config (Dict[str, str]): Configuration for the language model.
            workflow_id (Optional[str]): ID for the workflow. Only used by main supervisor.
            is_assistant (bool): Whether this is an assistant supervisor.
            system_message (Optional[str]): The initial system message for the agent.
            use_agents (bool): Whether to use agents or not.

        Raises:
            ValueError: If the name is empty or if workflow management rules are violated.
        """
        super().__init__(llm_config=llm_config)

        if not name:
            raise ValueError("Supervisor name cannot be empty")

        self.name = name
        self.is_assistant = is_assistant
        self.workflow_id = workflow_id

        self._pending_registrations: List[Union[Agent, 'Supervisor']] = []
        self.system_message = system_message if system_message is not None else self._get_default_system_message()

        if not is_assistant:
            if workflow_id:
                try:
                    self.history_manager = HistoryManager(workflow_id)
                except:
                    self._initialize_workflow()
                    self.history_manager = HistoryManager(workflow_id)
                if not self.history_manager.has_system_message(self.name):
                    self._initialize_chat_history()
            else:
                self._initialize_workflow()
                self.history_manager = HistoryManager(self.workflow_id)
                self._initialize_chat_history()
        else:
            self.history_manager = None

        self.registered_agents: List[Union[Agent, 'Supervisor']] = []
        self.available_tools: List[Dict[str, Any]] = []
        self.use_agents = use_agents

        self.chat_history: List[Dict[str, str]] = []

        self.debugger = Debugger(name=self.name, workflow_id=self.workflow_id)
        self.debugger.start_session()

    def _emit_event(self, on_event: Optional[Callable], event_type: str, data: Dict[str, Any]):
        """Safely emits an event if the callback is provided."""
        if on_event:
            payload = {
                "id": f"evt_{uuid.uuid4()}",
                "timestamp": datetime.utcnow().isoformat() + "Z",
                "type": event_type,
                "data": data,
            }
            on_event(payload)

    def _get_default_system_message(self) -> str:
        """
        Get the default system message based on supervisor type.

        Returns:
            str: Appropriate system message for the supervisor type.
        """
        if self.is_assistant:
            return """You are an Assistant Supervisor, part of a larger workflow managed by the Main Supervisor. 
            Your role is to handle specialized tasks delegated to you and manage your assigned agents effectively."""
        else:
            return """You are the Main Supervisor, responsible for managing the entire workflow. 
            Your tasks include coordinating with Assistant Supervisors and direct agents, ensuring efficient task delegation 
            and execution."""

    def _initialize_chat_history(self) -> None:
        """Initialize chat history with system message and record in history manager."""
        if self.system_message:
            system_msg = {'role': 'system', 'content': self.system_message}
            self.chat_history = [system_msg]

            if self.history_manager:
                self.history_manager.append_message(message=system_msg,
                                                    sender_type=EntityType.MAIN_SUPERVISOR
                                                    if not self.is_assistant else EntityType.ASSISTANT_SUPERVISOR,
                                                    sender_name=self.name)

    def configure_system_prompt(self, system_prompt: str) -> None:
        """
        Configure the system prompt for the Supervisor.

        Args:
            system_prompt (str): The new system prompt to set.
        """
        self.system_message = {"role": "system", "content": system_prompt}

    def register_agent(self, agent: Union[Agent, 'Supervisor']) -> None:
        """
        Register a new agent or assistant supervisor.

        Args:
            agent (Union[Agent, Supervisor]): The agent or assistant supervisor to register.

        Raises:
            ValueError: If attempting to register a main supervisor or if registration rules are violated.
        """

        if isinstance(agent, Supervisor) and not agent.is_assistant:
            raise ValueError("Only assistant supervisors can be registered as agents")

        if self.is_assistant and not self.workflow_id:
            self._pending_registrations.append(agent)
            return

        if isinstance(agent, Supervisor):
            agent.set_workflow_id(self.workflow_id)
        else:
            agent.set_workflow_id(self.workflow_id)

        self.registered_agents.append(agent)
        self._add_agent_tool(agent)
        # self.system_message += f"{agent.name}: {agent.system_message}\n"

    def set_workflow_id(self, workflow_id: str) -> None:
        """
        Set the workflow ID for this supervisor.
        
        Args:
            workflow_id (str): The workflow ID to set.
        """
        self.workflow_id = workflow_id
        self.debugger.update_workflow_id(workflow_id)
        self.history_manager = HistoryManager(workflow_id)
        self._initialize_chat_history()
        self._process_pending_registrations()

    def _process_pending_registrations(self) -> None:
        """Process any pending agent registrations."""
        if self._pending_registrations:
            for agent in self._pending_registrations:
                self.register_agent(agent)
            self._pending_registrations.clear()

    def _add_agent_tool(self, agent: Agent) -> None:
        """
        Add a tool for the registered agent to the available tools.

        Args:
            agent (Agent): The agent for which to add a tool.
        """
        self.available_tools.append({
            "type": "function",
            "function": {
                "name": f"delegate_to_{agent.name}",
                "description": agent.system_message,
                "parameters": {
                    "type": "object",
                    "properties": {
                        "reasoning": {
                            "type":
                                "string",
                            "description": (f"Supervisor's reasoning for choosing the {agent.name} agent. "
                                            "Explain why this agent is being invoked and what is expected of it.")
                        },
                        "query": {
                            "type": "string",
                            "description": (f"The actual query or instruction for {agent.name} agent to respond to.")
                        },
                        "context": {
                            "type":
                                "string",
                            "description": ("All relevant background information, prior facts, decisions, "
                                            "and state needed by the agent to solve the current query. "
                                            "Should be as detailed and self-contained as possible.")
                        },
                    },
                    "required": ["reasoning", "query", "context"]
                }
            },
            "strict": True,
        })

    def _initialize_workflow(self) -> None:
        """
        Initialize the workflow directory and set up necessary structures.
        Only called by main supervisor.

        Raises:
            ValueError: If an assistant supervisor attempts to initialize a workflow.
        """
        if self.is_assistant:
            raise ValueError("Assistant supervisors cannot initialize workflows")

        if not self.workflow_id:
            self.workflow_id = str(uuid.uuid4())

        workflow_path = Path("xronai_logs") / self.workflow_id
        workflow_path.mkdir(parents=True, exist_ok=True)

        history_file = workflow_path / "history.jsonl"
        if not history_file.exists():
            history_file.touch()

    def get_registered_agents(self) -> List[str]:
        """
        Get the names of all registered agents.

        Returns:
            List[str]: A list of registered agent names.
        """
        return [agent.name for agent in self.registered_agents]

    def delegate_to_agent(self,
                          message: ChatCompletionMessage,
                          parent_msg_id: str,
                          supervisor_chain: Optional[List[str]] = None,
                          on_event: Optional[Callable] = None) -> str:
        """
        Delegate a task to the appropriate agent based on the supervisor's response.

        Args:
            message (ChatCompletionMessage): The message containing the delegation information.
            parent_msg_id (str): ID of the parent message in history.
            supervisor_chain (Optional[List[str]]): Chain of supervisors involved in delegation.
            on_event (Optional[Callable]): The event callback function.

        Returns:
            str: The response from the delegated agent.

        Raises:
            ValueError: If no matching agent is found for delegation or if the message structure is unexpected.
        """
        if not hasattr(message, 'tool_calls') or not message.tool_calls:
            raise ValueError("Message does not contain tool calls")

        function_call = message.tool_calls[0]
        target_agent_name = function_call.function.name.replace("delegate_to_", "")
        args = json.loads(function_call.function.arguments)
        reasoning = args.get('reasoning')
        context = args.get('context')
        query = args.get('query')

        if not query:
            raise ValueError("Query is missing from the function call")

        target_agent = next((agent for agent in self.registered_agents if agent.name == target_agent_name), None)

        if not target_agent:
            raise ValueError(f"No agent found with name '{target_agent_name}'")

        self.debugger.log(f"[DELEGATION] Agent: {target_agent_name}")
        self.debugger.log(f"[REASONING] {reasoning}")
        self.debugger.log(f"[CONTEXT] {context}")
        self.debugger.log(f"[QUERY] {query}")

        self._emit_event(
            on_event, "SUPERVISOR_DELEGATE", {
                "source": {
                    "name": self.name,
                    "type": "ASSISTANT_SUPERVISOR" if self.is_assistant else "SUPERVISOR"
                },
                "target": {
                    "name": target_agent.name,
                    "type": "AGENT" if isinstance(target_agent, Agent) else "ASSISTANT_SUPERVISOR"
                },
                "reasoning": reasoning,
                "query_for_agent": query
            })

        current_chain = supervisor_chain or []
        current_chain.append(self.name)

        agent_response = target_agent.chat(query=f"CONTEXT:\n{context}\n\nQUERY:\n{query}",
                                           sender_name=self.name,
                                           on_event=on_event)
        self.debugger.log(f"[RESPONSE] {target_agent_name}: {agent_response}")
        return agent_response

    def chat(self,
             query: str,
             sender_name: Optional[str] = None,
             supervisor_chain: Optional[List[str]] = None,
             on_event: Optional[Callable] = None) -> str:
        """
        Process user input and generate a response using the appropriate agents.

        Args:
            query (str): The user's input query.
            sender_name (Optional[str]): Name of the sender (for assistant supervisors).
            supervisor_chain (Optional[List[str]]): Chain of supervisors in delegation.
            on_event (Optional[Callable]): A callback function to stream events to.

        Returns:
            str: The final response to the user's query.

        Raises:
            RuntimeError: If there's an error in processing the user input.
        """
        self.debugger.log(f"[USER INPUT] {query}")

        self._emit_event(on_event, "WORKFLOW_START", {"user_query": query})

        current_chain = supervisor_chain or []
        if self.name not in current_chain:
            current_chain.append(self.name)

        user_msg = {'role': 'user', 'content': query}
        self.chat_history.append(user_msg)

        user_msg_id = self.history_manager.append_message(
            message=user_msg,
            sender_type=EntityType.MAIN_SUPERVISOR if sender_name else EntityType.USER,
            sender_name=sender_name or "user",
            supervisor_chain=current_chain)

        try:
            while True:
                supervisor_response = self.generate_response(self.chat_history,
                                                             tools=self.available_tools,
                                                             use_tools=self.use_agents).choices[0]

                if not supervisor_response.finish_reason == "tool_calls":
                    query_answer = supervisor_response.message.content
                    self.debugger.log(f"[SUPERVISOR RESPONSE] {query_answer}")

                    response_msg = {"role": "assistant", "content": query_answer}
                    self.chat_history.append(response_msg)

                    self.history_manager.append_message(message=response_msg,
                                                        sender_type=EntityType.MAIN_SUPERVISOR
                                                        if not self.is_assistant else EntityType.ASSISTANT_SUPERVISOR,
                                                        sender_name=self.name,
                                                        parent_id=user_msg_id,
                                                        supervisor_chain=current_chain)

                    self._emit_event(
                        on_event, "FINAL_RESPONSE", {
                            "source": {
                                "name": self.name,
                                "type": "ASSISTANT_SUPERVISOR" if self.is_assistant else "SUPERVISOR"
                            },
                            "content": query_answer
                        })
                    self._emit_event(on_event, "WORKFLOW_END", {})
                    return query_answer

                tool_call = supervisor_response.message.tool_calls[0]
                tool_msg = {
                    "role":
                        "assistant",
                    "content":
                        None,
                    "tool_calls": [{
                        'id': tool_call.id,
                        'type': 'function',
                        'function': {
                            'name': tool_call.function.name,
                            'arguments': tool_call.function.arguments
                        }
                    }]
                }
                self.chat_history.append(tool_msg)

                tool_msg_id = self.history_manager.append_message(
                    message=tool_msg,
                    sender_type=EntityType.MAIN_SUPERVISOR
                    if not self.is_assistant else EntityType.ASSISTANT_SUPERVISOR,
                    sender_name=self.name,
                    parent_id=user_msg_id,
                    tool_call_id=tool_call.id,
                    supervisor_chain=current_chain)

                if hasattr(supervisor_response.message, 'tool_calls') and supervisor_response.message.tool_calls:
                    agent_feedback = self.delegate_to_agent(supervisor_response.message,
                                                            tool_msg_id,
                                                            supervisor_chain=current_chain,
                                                            on_event=on_event)

                    self._emit_event(
                        on_event,
                        "AGENT_RESPONSE",
                        {
                            "source": {
                                "name": tool_call.function.name.replace("delegate_to_", ""),
                                "type": "AGENT"
                            },  # A bit of a hack to get agent name
                            "content": agent_feedback
                        })

                    feedback_msg = {"role": "tool", "content": agent_feedback, "tool_call_id": tool_call.id}
                    self.chat_history.append(feedback_msg)

                    self.history_manager.append_message(message=feedback_msg,
                                                        sender_type=EntityType.TOOL,
                                                        sender_name=self.name,
                                                        parent_id=tool_msg_id,
                                                        tool_call_id=tool_call.id,
                                                        supervisor_chain=current_chain)
                else:
                    final_content = supervisor_response.message.content
                    self._emit_event(
                        on_event, "FINAL_RESPONSE", {
                            "source": {
                                "name": self.name,
                                "type": "ASSISTANT_SUPERVISOR" if self.is_assistant else "SUPERVISOR"
                            },
                            "content": final_content
                        })
                    self._emit_event(on_event, "WORKFLOW_END", {})
                    return final_content

        except Exception as e:
            error_msg = f"Error in processing user input: {str(e)}"
            self.debugger.log(f"[ERROR] {error_msg}", level="error")
            self._emit_event(
                on_event, "ERROR", {
                    "source": {
                        "name": self.name,
                        "type": "ASSISTANT_SUPERVISOR" if self.is_assistant else "SUPERVISOR"
                    },
                    "error_message": error_msg
                })
            self._emit_event(on_event, "WORKFLOW_END", {})
            raise RuntimeError(error_msg)

    def start_interactive_session(self) -> None:
        """
        Start an interactive session with the user.

        This method initiates a loop that continuously processes user input
        until the user decides to exit.
        """
        print("Starting interactive session. Type 'exit' to end the session.")
        while True:
            user_input = input("User: ").strip()
            if user_input.lower() == "exit":
                print("Ending session. Goodbye!")
                break
            try:
                supervisor_output = self.chat(query=user_input)
                print(f"Supervisor: {supervisor_output}")
            except Exception as e:
                print(f"An error occurred: {str(e)}")

    def __str__(self) -> str:
        """Return a string representation of the Supervisor instance."""
        return f"Supervisor(name={self.name}, agents={len(self.registered_agents)})"

    def __repr__(self) -> str:
        """Return a detailed string representation of the Supervisor instance."""
        return (f"Supervisor(name={self.name}, llm_config={self.llm_config}, "
                f"registered_agents={[agent.name for agent in self.registered_agents]})")

    def reset_chat_history(self) -> None:
        """Reset chat history to initial state and clear history manager."""
        self.history_manager.clear_history()
        self._initialize_chat_history()

    def get_chat_history(self) -> List[Dict[str, str]]:
        """
        Get the current chat history.

        Returns:
            List[Dict[str, str]]: The current chat history.
        """
        return self.chat_history

    def add_to_chat_history(self, role: str, content: str) -> None:
        """
        Add a new message to both chat history and history manager.

        Args:
            role (str): The role of the message sender (e.g., 'user', 'assistant', 'system').
            content (str): The content of the message.

        Raises:
            ValueError: If an invalid role is provided.
        """
        if role not in ['user', 'assistant', 'system', 'tool']:
            raise ValueError(f"Invalid role: {role}")

        message = {"role": role, "content": content}
        self.chat_history.append(message)

        sender_type = {
            'user': EntityType.USER,
            'assistant': EntityType.MAIN_SUPERVISOR if not self.is_assistant else EntityType.ASSISTANT_SUPERVISOR,
            'system': EntityType.MAIN_SUPERVISOR if not self.is_assistant else EntityType.ASSISTANT_SUPERVISOR,
            'tool': EntityType.TOOL
        }[role]

        self.history_manager.append_message(message=message, sender_type=sender_type, sender_name=self.name)

    def get_agent_by_name(self, agent_name: str) -> Optional[Agent]:
        """
        Get a registered agent by its name.

        Args:
            agent_name (str): The name of the agent to retrieve.

        Returns:
            Optional[Agent]: The agent with the specified name, or None if not found.
        """
        return next((agent for agent in self.registered_agents if agent.name.lower() == agent_name.lower()), None)

    def remove_agent(self, agent_name: str) -> bool:
        """
        Remove a registered agent by its name.

        Args:
            agent_name (str): The name of the agent to remove.

        Returns:
            bool: True if the agent was successfully removed, False otherwise.
        """
        agent = self.get_agent_by_name(agent_name)
        if agent:
            self.registered_agents.remove(agent)
            self.available_tools = [
                tool for tool in self.available_tools if tool['function']['name'] != f"delegate_to_{agent_name}"
            ]
            return True
        return False

    def update_system_message(self) -> None:
        """
        Update the system message to reflect the current set of registered agents.
        """
        agent_descriptions = "\n".join(f"{agent.name}: {agent.system_message}" for agent in self.registered_agents)
        self.system_message = f"{self._get_default_system_message()}\n\n{agent_descriptions}"
        self.reset_chat_history()

    @property
    def is_main_supervisor(self) -> bool:
        """
        Check if this is the main supervisor.

        Returns:
            bool: True if this is the main supervisor, False if assistant.
        """
        return not self.is_assistant

    def get_workflow_info(self) -> Dict[str, Any]:
        """
        Get information about the current workflow.

        Returns:
            Dict[str, Any]: Dictionary containing workflow information.
        """
        return {
            'workflow_id':
                self.workflow_id,
            'supervisor_type':
                'main' if self.is_main_supervisor else 'assistant',
            'name':
                self.name,
            'registered_agents': [{
                'name': agent.name,
                'type': 'supervisor' if isinstance(agent, Supervisor) else 'agent'
            } for agent in self.registered_agents]
        }

    def display_agent_graph(self, indent="", skip_header=False) -> None:
        """
        Display the supervisor-agent hierarchy.
        
        Args:
            indent (str): Current indentation level
            skip_header (bool): Whether to skip printing the supervisor header
        """
        if not skip_header:
            supervisor_type = "Main Supervisor" if self.is_main_supervisor else "Assistant Supervisor"
            print(f"{indent}{supervisor_type}: {self.name}")

            if self.registered_agents:
                print(f"{indent}│")

        for i, agent in enumerate(self.registered_agents):
            is_last_agent = i == len(self.registered_agents) - 1
            agent_prefix = "└── " if is_last_agent else "├── "
            current_indent = indent + ("    " if is_last_agent else "│   ")

            if isinstance(agent, Supervisor):
                print(f"{indent}{agent_prefix}Assistant Supervisor: {agent.name}")
                agent.display_agent_graph(current_indent, skip_header=True)  # Skip header for recursive calls
            else:
                print(f"{indent}{agent_prefix}Agent: {agent.name}")
                if hasattr(agent, 'tools') and agent.tools:
                    for j, tool in enumerate(agent.tools):
                        is_last_tool = j == len(agent.tools) - 1
                        tool_prefix = "└── " if is_last_tool else "├── "
                        tool_name = tool['metadata']['function']['name'] if 'metadata' in tool else "Unnamed Tool"
                        print(f"{current_indent}{tool_prefix}Tool: {tool_name}")
                else:
                    print(f"{current_indent}└── No tools available")

            if not is_last_agent and i < len(self.registered_agents) - 1:
                print(f"{indent}│")



================================================
FILE: src/xronai/history/__init__.py
================================================
from .history_manager import HistoryManager, EntityType

__all__ = ['HistoryManager', 'EntityType']



================================================
FILE: src/xronai/history/history_manager.py
================================================
"""
This module provides functionality for managing conversation history in hierarchical
AI workflows. It handles storage and retrieval of messages between users,
supervisors, agents, and tools within a workflow.

The module uses a JSONL-based storage system where each workflow's conversation
history is stored in a dedicated file, supporting message threading, delegation
chains, and relationship tracking between different entities.

Components:
    EntityType: Enum for different entity types (USER, MAIN_SUPERVISOR, etc.)
    HistoryManager: Main class for handling history operations

Structure:
    xronai_logs/{workflow_id}/history.jsonl

Note:
    Workflow directory must be initialized by a main supervisor before use.
"""

import os, collections
import json
import uuid
from datetime import datetime
from typing import Dict, List, Optional, Any, Union
from pathlib import Path
from enum import Enum


class EntityType(str, Enum):
    """Enumeration of entity types in the workflow system."""
    USER = "user"
    MAIN_SUPERVISOR = "main_supervisor"
    ASSISTANT_SUPERVISOR = "assistant_supervisor"
    AGENT = "agent"
    TOOL = "tool"


class HistoryManager:
    """
    Manages conversation history for workflows involving supervisors, agents, and tools.
    
    This class works alongside the chat_history in Supervisor and Agent classes,
    providing persistent storage and advanced querying capabilities while maintaining
    the conversation structure and relationships.

    Attributes:
        workflow_id (str): Unique identifier for the workflow
        workflow_path (Path): Path to the workflow directory
        history_file (Path): Path to the JSONL file storing the conversation history
    """

    def __init__(self, workflow_id: str):
        """
        Initialize the HistoryManager.

        Args:
            workflow_id (str): Unique identifier for the workflow.
                             Must be provided by a main supervisor.

        Raises:
            ValueError: If workflow_id is None or workflow directory doesn't exist.
        """
        if not workflow_id:
            raise ValueError("workflow_id must be provided")

        self.workflow_id = workflow_id
        self.workflow_path = Path("xronai_logs") / self.workflow_id
        self.history_file = self.workflow_path / "history.jsonl"

        if not self.workflow_path.exists():
            raise ValueError(f"Workflow directory does not exist: {self.workflow_path}. "
                             "It should be created by the main supervisor.")

    def append_message(self,
                       message: Dict[str, Any],
                       sender_type: EntityType,
                       sender_name: str,
                       parent_id: Optional[str] = None,
                       tool_call_id: Optional[str] = None,
                       supervisor_chain: Optional[List[str]] = None) -> str:
        """
        Append a message to the conversation history.
        
        This method is called alongside chat_history updates to maintain
        persistent storage of the conversation.

        Args:
            message (Dict[str, Any]): The message to append (same format as chat_history)
            sender_type (EntityType): Type of the sender
            sender_name (str): Name of the sender
            parent_id (Optional[str]): ID of the parent message in conversation
            tool_call_id (Optional[str]): ID of related tool call if applicable
            supervisor_chain (Optional[List[str]]): List of supervisors in the delegation chain

        Returns:
            str: Generated message ID for reference in future messages

        Example:
            >>> msg_id = history_manager.append_message(
            ...     message={"role": "user", "content": "Hello"},
            ...     sender_type=EntityType.USER,
            ...     sender_name="user"
            ... )
            
            >>> msg_id = history_manager.append_message(
            ...     message={"role": "assistant", "content": "Process data"},
            ...     sender_type=EntityType.MAIN_SUPERVISOR,
            ...     sender_name="MainSupervisor",
            ...     supervisor_chain=["MainSupervisor", "AssistantSupervisor"]
            ... )
        """
        message_id = str(uuid.uuid4())

        # Prepare entry with metadata
        entry = {
            'message_id': message_id,
            'timestamp': datetime.utcnow().isoformat(),
            'workflow_id': self.workflow_id,
            'sender_type': sender_type,
            'sender_name': sender_name,
            'parent_id': parent_id,
            'tool_call_id': tool_call_id,
            'supervisor_chain': supervisor_chain or [],  # Empty list if None
            **message  # Include original message fields
        }

        # Append to history file
        with open(self.history_file, 'a') as f:
            f.write(json.dumps(entry) + '\n')

        return message_id

    def load_chat_history(self, entity_name: str) -> List[Dict[str, Any]]:
        """
        Load and reconstruct the LLM-compatible conversation history for a given entity (supervisor or agent).

        This function extracts only those messages from the full workflow history that were 
        truly exchanged with or delegated to this entity, ensuring that synthetic user prompts, 
        agent LLM responses, tool calls, and tool results are correctly ordered and threaded 
        as would be expected by any LLM for conversation continuation. Irrelevant messages 
        intended for other agents are excluded.

        Args:
            entity_name (str): The name of the entity for which to load chat history 
                (e.g. supervisor, assistant supervisor, or agent).

        Returns:
            List[Dict[str, Any]]: Ordered list of chat messages in the expected chat_history 
                format for initializing or restoring the entity's LLM context.

        Example:
            >>> agent.chat_history = history_manager.load_chat_history("AgentName")
        """
        if not self.history_file.exists():
            return []

        with open(self.history_file, "r") as f:
            all_msgs = [json.loads(line) for line in f]

        system = next((m for m in all_msgs if m["role"] == "system" and m["sender_name"] == entity_name), None)
        history = []
        if system:
            history.append(self._format_for_chat_history(system))

        delegated_user_msgs = []
        for m in all_msgs:
            if m["role"] == "user":
                if m.get("supervisor_chain") and m["supervisor_chain"] and m["supervisor_chain"][-1] == entity_name:
                    delegated_user_msgs.append(m)
                else:
                    if any(n["role"] == "assistant" and n["sender_name"] == entity_name and
                           n.get("parent_id") == m["message_id"] for n in all_msgs):
                        delegated_user_msgs.append(m)

        delegated_user_msgs.sort(key=lambda x: x["timestamp"])

        for user_msg in delegated_user_msgs:
            history.append(self._format_for_chat_history(user_msg))

            queue = collections.deque()

            children = [m for m in all_msgs if m.get("parent_id") == user_msg["message_id"]]
            children = [m for m in children if m["sender_name"] == entity_name or m["role"] == "tool"]
            children.sort(key=lambda x: x["timestamp"])

            for ch in children:
                queue.append(ch)
            while queue:
                msg = queue.popleft()
                formatted = self._format_for_chat_history(msg)
                if formatted not in history:
                    history.append(formatted)
                if msg["role"] == "assistant" and msg.get("tool_calls"):
                    for tool_call in msg["tool_calls"]:
                        tool_msgs = [
                            t for t in all_msgs if t["role"] == "tool" and t.get("tool_call_id") == tool_call["id"] and
                            t.get("parent_id") == msg["message_id"]
                        ]
                        tool_msgs.sort(key=lambda x: x["timestamp"])
                        for tmsg in tool_msgs:
                            tfmt = self._format_for_chat_history(tmsg)
                            if tfmt not in history:
                                history.append(tfmt)

                            wrapups = [
                                mm for mm in all_msgs
                                if mm.get("parent_id") == tmsg["message_id"] and mm["sender_name"] == entity_name
                            ]
                            wrapups.sort(key=lambda x: x["timestamp"])
                            for wmsg in wrapups:
                                queue.append(wmsg)

        return history

    def get_frontend_history(self) -> List[Dict[str, Any]]:
        """
        Get complete conversation history formatted for frontend display.

        Returns:
            List[Dict[str, Any]]: Complete conversation history with proper threading

        Example:
            >>> history = history_manager.get_frontend_history()
        """
        if not self.history_file.exists():
            return []

        messages = []
        with open(self.history_file, 'r') as f:
            messages = [json.loads(line) for line in f]

        # Add delegation chain information for display
        for msg in messages:
            if msg.get('supervisor_chain'):
                msg['delegation_path'] = ' → '.join(msg['supervisor_chain'])

            # Format display name based on sender type and chain
            if msg['sender_type'] in [EntityType.MAIN_SUPERVISOR, EntityType.ASSISTANT_SUPERVISOR]:
                msg['display_name'] = f"{msg['sender_type']}: {msg['sender_name']}"
                if msg.get('supervisor_chain'):
                    msg['display_name'] += f" ({msg['delegation_path']})"
            else:
                msg['display_name'] = ("User" if msg['sender_type'] == EntityType.USER else msg['sender_name'])

        return self._build_conversation_thread(messages)

    def _format_for_chat_history(self, msg: Dict[str, Any]) -> Dict[str, Any]:
        """
        Format a raw persisted message as an LLM-compatible chat turn.

        Converts a stored message (from history.jsonl) into the minimal 
        chat message dict expected by the LLM OpenAI-compatible API: 
        always includes 'role' and 'content', and optionally adds 
        'tool_calls' (for assistant tool call steps) or 'tool_call_id' and 
        'name' (for tool response steps).

        Args:
            msg (Dict[str, Any]): Raw message object loaded from the workflow history.

        Returns:
            Dict[str, Any]: A minimal chat message ready for LLM dialog replay,
                compatible with openai.ChatCompletion and similar APIs.

        Returns Example:
            # For role='assistant'
            {'role': 'assistant', 'content': '...' [, 'tool_calls': [...] ]}
            # For role='tool'
            {'role': 'tool', 'content': '42', 'tool_call_id': 'call_xyz', 'name': 'calculate'}
        """
        formatted = {
            "role": msg["role"],
            "content": msg.get("content", ""),
        }
        if "tool_calls" in msg and msg["tool_calls"]:
            formatted["tool_calls"] = msg["tool_calls"]
        if msg["role"] == "tool":
            formatted["tool_call_id"] = msg.get("tool_call_id")
            formatted["name"] = msg["sender_name"]  # who returned tool output
        return formatted

    def has_system_message(self, entity_name: str) -> bool:
        """
        Check if system message exists for an entity in the current workflow.
        
        Args:
            entity_name (str): Name of the entity to check
            
        Returns:
            bool: True if system message exists, False otherwise
        """
        if not self.history_file.exists():
            return False

        with open(self.history_file, 'r') as f:
            for line in f:
                msg = json.loads(line)
                if (msg['role'] == 'system' and msg['sender_name'] == entity_name and
                        msg['workflow_id'] == self.workflow_id):
                    return True
        return False

    def _sort_messages(self, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Sort messages based on their relationships and timestamps.

        Args:
            messages (List[Dict[str, Any]]): Messages to sort

        Returns:
            List[Dict[str, Any]]: Sorted messages
        """
        return sorted(messages, key=lambda x: x.get('timestamp', ''))

    def _build_conversation_thread(self, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Build a threaded conversation structure.

        Args:
            messages (List[Dict[str, Any]]): Raw messages from history

        Returns:
            List[Dict[str, Any]]: Threaded conversation structure
        """
        messages = self._sort_messages(messages)

        threaded = []
        message_map = {}

        for msg in messages:
            msg_copy = msg.copy()
            msg_copy['responses'] = []
            message_map[msg['message_id']] = msg_copy

            if msg['parent_id'] and msg['parent_id'] in message_map:
                message_map[msg['parent_id']]['responses'].append(msg_copy)
            else:
                threaded.append(msg_copy)

        return threaded

    def clear_history(self) -> None:
        """Clear the entire conversation history for the current workflow."""
        if self.history_file.exists():
            self.history_file.unlink()
            self.history_file.touch()

    def get_messages_by_entity(self, entity_name: str) -> List[Dict[str, Any]]:
        """
        Get all messages related to a specific entity.

        Args:
            entity_name (str): Name of the entity

        Returns:
            List[Dict[str, Any]]: All messages related to the entity
        """
        messages = []
        with open(self.history_file, 'r') as f:
            for line in f:
                msg = json.loads(line)
                if msg['sender_name'] == entity_name:
                    messages.append(msg)
        return self._sort_messages(messages)



================================================
FILE: src/xronai/utils/__init__.py
================================================
from .debugger import Debugger

__all__ = ["Debugger"]



================================================
FILE: src/xronai/utils/debugger.py
================================================
"""
Debugger module for logging and debugging AI interactions.

This module provides a Debugger class that handles logging for agents and supervisors,
organizing logs within the xronai_logs directory structure when part of a workflow,
or in a standalone logs directory when used independently.
"""

import logging
import json
from pathlib import Path
from typing import Optional, Dict, List


class Debugger:
    """
    A debug logging utility for AI agents and supervisors.

    This class manages debug logging, organizing logs either within a workflow's
    directory structure or in a standalone logs directory. It supports various
    logging levels and formats for different types of data.

    Attributes:
        name (str): Name of the entity (agent/supervisor) being logged
        logger (logging.Logger): The logger instance for this entity
        log_file_path (Path): Path to the log file
    """

    STANDALONE_LOG_DIR = Path('xronai_logs') / 'standalone_logs'

    def __init__(self, name: str, workflow_id: Optional[str] = None, log_level: int = logging.DEBUG):
        """
        Initialize the Debugger instance.

        Args:
            name (str): Name of the entity (agent/supervisor) being logged
            workflow_id (Optional[str]): ID of the workflow if part of one
            log_level (int): Logging level (default: logging.DEBUG)
        """
        self.name = name
        self.workflow_id = workflow_id
        self.log_level = log_level
        self._setup_logger()

    def _setup_logger(self) -> None:
        """Set up or reconfigure the logger with current settings."""
        # Determine log directory
        if self.workflow_id:
            log_dir = Path('xronai_logs') / self.workflow_id / 'logs'
        else:
            log_dir = self.STANDALONE_LOG_DIR

        # Ensure log directory exists
        log_dir.mkdir(parents=True, exist_ok=True)

        # Set up log file path
        self.log_file_path = log_dir / f"{self.name}.log"

        # Create a unique logger name
        logger_name = self.name

        # Remove existing logger if it exists
        if logger_name in logging.root.manager.loggerDict:
            logging.root.manager.loggerDict.pop(logger_name)

        # Create new logger
        self.logger = logging.getLogger(logger_name)
        self.logger.setLevel(self.log_level)

        # Remove any existing handlers
        self.logger.handlers.clear()

        # Create and configure file handler
        file_handler = logging.FileHandler(self.log_file_path, mode='a')
        file_handler.setLevel(self.log_level)

        # Create formatter
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)

        # Add handler to logger
        self.logger.addHandler(file_handler)

        # Prevent propagation to root logger
        self.logger.propagate = False

    def update_workflow_id(self, workflow_id: str) -> None:
        """
        Update the workflow ID and move logs to the workflow directory.

        This method is called when an agent or assistant supervisor is
        registered with a workflow.

        Args:
            workflow_id (str): The new workflow ID
        """
        old_log_path = self.log_file_path
        self.workflow_id = workflow_id

        # Reconfigure logger with new workflow_id
        self._setup_logger()

        # Move existing logs if they exist
        if old_log_path.exists() and old_log_path != self.log_file_path:
            try:
                with open(old_log_path, 'r') as source:
                    with open(self.log_file_path, 'a') as dest:
                        dest.write('\n' + source.read())
                # Remove the old log file
                old_log_path.unlink()
            except Exception as e:
                self.log(f"Error moving logs: {str(e)}", level="error")

        # Log the transition
        self.log(f"Logging continued in workflow: {workflow_id}")

    def log(self, message: str, level: str = "info") -> None:
        """
        Log a message at the specified level.

        Args:
            message (str): The message to log
            level (str): The logging level (debug/info/warning/error/critical)
        """
        level = level.lower()
        if level == "debug":
            self.logger.debug(message)
        elif level == "info":
            self.logger.info(message)
        elif level == "warning":
            self.logger.warning(message)
        elif level == "error":
            self.logger.error(message)
        elif level == "critical":
            self.logger.critical(message)

        # Ensure message is written immediately
        for handler in self.logger.handlers:
            handler.flush()

    def log_dict(self, data: Dict, message: str = "") -> None:
        """
        Log a dictionary with optional message.

        Args:
            data (Dict): The dictionary to log
            message (str): Optional message to precede the dictionary
        """
        self.log(f"{message}\n{json.dumps(data, indent=2)}")

    def log_list(self, data: List, message: str = "") -> None:
        """
        Log a list with optional message.

        Args:
            data (List): The list to log
            message (str): Optional message to precede the list
        """
        self.log(f"{message}\n{json.dumps(data, indent=2)}")

    def start_session(self) -> None:
        """Log the start of a new session."""
        self.log(f"------ New Session Started for {self.name} ------")

    def end_session(self) -> None:
        """Log the end of the current session."""
        self.log(f"------ Session Ended for {self.name} ------")

    def __str__(self) -> str:
        """Return string representation of the Debugger instance."""
        return f"Debugger(name={self.name}, log_file={self.log_file_path})"

    def __repr__(self) -> str:
        """Return detailed string representation of the Debugger instance."""
        return (f"Debugger(name={self.name}, "
                f"log_file={self.log_file_path}, "
                f"level={self.log_level})")



================================================
FILE: studio/__init__.py
================================================
[Empty file]


================================================
FILE: studio/cli.py
================================================
import os
import typer
import uvicorn
import webbrowser
import asyncio
from typing_extensions import Annotated
from typing import Optional

app = typer.Typer(name="xronai", help="The command-line interface for the XronAI SDK.", add_completion=False)


@app.callback(invoke_without_command=True)
def main(ctx: typer.Context):
    if ctx.invoked_subcommand is None:
        print("Welcome to XronAI CLI. Please specify a command, e.g., 'studio'.")
        print(ctx.get_help())


@app.command()
def studio(config: Annotated[Optional[str],
                             typer.Option(help="Path to a workflow YAML configuration file to load.")] = None,
           host: Annotated[str, typer.Option(help="The host address to run the server on.")] = "127.0.0.1",
           port: Annotated[int, typer.Option(help="The port number to run the server on.")] = 8000,
           no_browser: Annotated[bool,
                                 typer.Option("--no-browser", help="Do not automatically open a web browser.")] = False,
           reload: Annotated[bool, typer.Option("--reload", help="Enable auto-reloading for development.")] = False):
    """
    Launches the XronAI Studio, a web-based UI for building and managing agentic workflows.
    """
    asyncio.run(start_studio_server(config=config, host=host, port=port, no_browser=no_browser, reload=reload))


async def start_studio_server(config, host, port, no_browser, reload):
    """
    The core async function to configure and run the Uvicorn server.
    """
    if config:
        os.environ["XRONAI_CONFIG_PATH"] = config
        print(f"INFO:     Will load configuration from: {config}")
    else:
        if "XRONAI_CONFIG_PATH" in os.environ:
            del os.environ["XRONAI_CONFIG_PATH"]

    print(f"INFO:     Starting XronAI Studio server...")
    base_url = f"http://{host}:{port}"
    print(f"INFO:     Studio will be available at {base_url}")

    uv_config = uvicorn.Config("studio.server.main:app", host=host, port=port, reload=reload, log_level="info")

    server = uvicorn.Server(uv_config)

    if not no_browser and not reload:

        async def open_browser_after_delay():
            await asyncio.sleep(1)
            webbrowser.open_new_tab(base_url)

        asyncio.create_task(open_browser_after_delay())

    await server.serve()


if __name__ == "__main__":
    app()



================================================
FILE: studio/server/__init__.py
================================================
[Empty file]


================================================
FILE: studio/server/graph_utils.py
================================================
from typing import Dict, List, Any, Union
from collections import defaultdict
from xronai.core import Supervisor, Agent

GRAPH_MARGIN_X = 50
GRAPH_MARGIN_Y = 500
X_SPACING = 350
Y_SPACING = 150


def build_graph_from_workflow(root_node: Union[Supervisor, Agent]) -> Dict[str, List[Dict[str, Any]]]:
    """
    Traverses a workflow and builds a JSON-serializable representation of the graph,
    including calculated positions for auto-layout.

    Args:
        root_node: The entry point of the workflow (a Supervisor or Agent).

    Returns:
        A dictionary containing lists of nodes and edges.
    """
    nodes = []
    edges = []
    processed_nodes = set()

    level_counts = defaultdict(int)
    node_levels = {}

    def calculate_positions(start_node: Union[Supervisor, Agent]):
        """First pass: determine the level of each node in the hierarchy."""
        q = [(start_node, 0)]
        visited = {start_node.name}

        while q:
            node, level = q.pop(0)
            node_levels[node.name] = level
            level_counts[level] += 1

            if isinstance(node, Supervisor):
                for child in node.registered_agents:
                    if child.name not in visited:
                        visited.add(child.name)
                        q.append((child, level + 1))

    def traverse_for_render(node: Union[Supervisor, Agent], current_y_offset: Dict[int, int]):
        """Second pass: build nodes and edges with calculated positions."""
        if not node or node.name in processed_nodes:
            return

        level = node_levels.get(node.name, 0)

        # Calculate position using the new margin controls
        pos_x = GRAPH_MARGIN_X + level * X_SPACING

        total_level_height = (level_counts[level] - 1) * Y_SPACING
        start_y = GRAPH_MARGIN_Y - (total_level_height / 2)
        pos_y = start_y + current_y_offset[level] * Y_SPACING

        current_y_offset[level] += 1

        node_type = 'supervisor' if isinstance(node, Supervisor) else 'agent'

        nodes.append({
            "id": node.name,
            "type": node_type,
            "pos_x": pos_x,
            "pos_y": pos_y,
            "data": {
                "title": node.name,
                "subtitle": "Manages agents" if node_type == 'supervisor' else "Performs tasks",
                "system_message": node.system_message or "Not set",
            }
        })
        processed_nodes.add(node.name)

        if isinstance(node, Supervisor):
            for child in node.registered_agents:
                edges.append({"source": node.name, "target": child.name})
                traverse_for_render(child, current_y_offset)

    calculate_positions(root_node)
    y_offsets = defaultdict(int)
    traverse_for_render(root_node, y_offsets)

    return {"nodes": nodes, "edges": edges}



================================================
FILE: studio/server/main.py
================================================
import logging
import asyncio
from contextlib import asynccontextmanager
from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException
from fastapi.staticfiles import StaticFiles

from xronai.core import Supervisor, Agent

from studio.server.state import StateManager
from studio.server.graph_utils import build_graph_from_workflow, GRAPH_MARGIN_X, GRAPH_MARGIN_Y, X_SPACING

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

state_manager = StateManager()


@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    FastAPI lifespan event handler. This is the professional way to
    run code on application startup and shutdown.
    """

    logger.info("Server is starting up...")
    await state_manager.load_workflow()
    yield

    logger.info("Server is shutting down...")


app = FastAPI(title="XronAI Studio Server",
              description="Backend server for the XronAI Studio.",
              version="0.1.0",
              lifespan=lifespan)


@app.get("/api/v1/status")
async def get_status():
    """
    Simple endpoint to check if the server is running.
    """
    root_node = state_manager.get_root_node()
    workflow_status = "loaded" if root_node else "not_loaded"
    root_node_name = root_node.name if root_node else "None"

    return {"status": "ok", "workflow_status": workflow_status, "root_node": root_node_name}


@app.get("/api/v1/workflow")
async def get_workflow_graph():
    """
    Returns a JSON representation of the currently loaded workflow graph.
    """
    root_node = state_manager.get_root_node()
    if not root_node:
        return {"nodes": [], "edges": []}

    user_node = {
        "id": "user-entry",
        "type": "user",
        "pos_x": GRAPH_MARGIN_X,
        "pos_y": GRAPH_MARGIN_Y + 150,
        "data": {
            "title": "User",
            "subtitle": "Workflow entry point"
        }
    }

    if state_manager.is_default_workflow():
        agent_node = {
            "id": root_node.name,
            "type": "agent",
            "pos_x": GRAPH_MARGIN_X + X_SPACING,
            "pos_y": GRAPH_MARGIN_Y + 150,
            "data": {
                "title": root_node.name,
                "subtitle": "Performs tasks"
            }
        }
        return {"nodes": [user_node, agent_node], "edges": [{"source": "user-entry", "target": root_node.name}]}

    graph = build_graph_from_workflow(root_node)

    for node in graph["nodes"]:
        node["pos_x"] += X_SPACING

    graph["nodes"].insert(0, user_node)
    graph["edges"].insert(0, {"source": "user-entry", "target": root_node.name})

    return graph


@app.get("/api/v1/nodes/{node_id}")
async def get_node_details(node_id: str):
    """
    Returns the detailed configuration of a specific node by its ID (name).
    """
    if node_id == "user-entry":
        return {
            "name": "User",
            "type": "user",
            "system_message": "This is the starting point of the workflow. It represents the user's input.",
            "tools": []
        }

    node = state_manager.find_node_by_id(node_id)
    if not node:
        raise HTTPException(status_code=404, detail=f"Node with ID '{node_id}' not found.")

    node_type = "supervisor" if isinstance(node, Supervisor) else "agent"

    # Extract tool names if the node is an Agent and has tools
    tools_list = []
    if isinstance(node, Agent) and node.tools:
        tools_list = [tool['metadata']['function']['name'] for tool in node.tools]

    return {"name": node.name, "type": node_type, "system_message": node.system_message, "tools": tools_list}


@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """
    The main WebSocket endpoint for real-time communication with the UI.
    """
    await websocket.accept()
    logger.info("WebSocket connection established.")

    chat_entry_point = state_manager.get_root_node()
    loop = asyncio.get_running_loop()

    if not chat_entry_point:
        logger.error("No workflow loaded. Cannot handle chat.")
        return

    def on_event_sync(event: dict):
        """
        This function is called by the XronAI core from a worker thread.
        It safely schedules the async `send_json` to run on the main event loop.
        """
        asyncio.run_coroutine_threadsafe(websocket.send_json(event), loop)

    try:
        while True:
            user_query = await websocket.receive_text()
            logger.info(f"Received user query: {user_query}")

            asyncio.create_task(asyncio.to_thread(chat_entry_point.chat, query=user_query, on_event=on_event_sync))

    except WebSocketDisconnect:
        logger.info("WebSocket connection closed.")
    except Exception as e:
        logger.error(f"An unexpected error occurred in the WebSocket: {e}", exc_info=True)
    finally:
        if not websocket.client_state.DISCONNECTED:
            await websocket.close()
        logger.info("WebSocket connection cleaned up.")


app.mount("/", StaticFiles(directory="studio/ui", html=True), name="ui")



================================================
FILE: studio/server/state.py
================================================
import os
import logging
from typing import Optional, Union
from dotenv import load_dotenv

from xronai.core import Supervisor, Agent
from xronai.config import AgentFactory, load_yaml_config

load_dotenv()

logger = logging.getLogger(__name__)


class StateManager:
    """
    Manages the in-memory state of the XronAI workflow for the studio server.
    """

    def __init__(self):
        self.root_node: Optional[Union[Supervisor, Agent]] = None
        self.is_default: bool = False
        self.config_path: Optional[str] = os.getenv("XRONAI_CONFIG_PATH")

    async def load_workflow(self):
        """
        Loads the workflow into memory from a config file or creates a default.
        """
        if self.config_path and os.path.exists(self.config_path):
            try:
                logger.info(f"Loading workflow from configuration: {self.config_path}")
                config = load_yaml_config(self.config_path)
                self.root_node = await AgentFactory.create_from_config(config)
                self.is_default = False
                logger.info(f"Successfully loaded supervisor: {self.root_node.name}")
            except Exception as e:
                logger.error(f"Failed to load workflow from {self.config_path}: {e}", exc_info=True)
                await self._load_default_workflow()
        else:
            logger.info("No configuration file provided. Loading default blank canvas workflow.")
            await self._load_default_workflow()

    async def _load_default_workflow(self):
        """
        Creates a simple, single-agent workflow as a default starter template.
        """
        self.root_node = Agent(
            name="DefaultAgent",
            llm_config={
                "model": os.getenv("LLM_MODEL", "default-model"),
                "api_key": os.getenv("LLM_API_KEY", "default-key"),
                "base_url": os.getenv("LLM_BASE_URL", "default-url"),
            },
            workflow_id="blank-canvas-session",
            system_message="You are a helpful assistant. You are the starting point for a new workflow.")
        self.is_default = True
        await self.root_node._load_mcp_tools()
        logger.info("Loaded default single-agent workflow.")

    def get_root_node(self) -> Optional[Union[Supervisor, Agent]]:
        return self.root_node

    def find_node_by_id(self, node_id: str) -> Optional[Union[Supervisor, Agent]]:
        """
        Finds a node (Agent or Supervisor) within the loaded workflow by its unique ID (name).
        
        Args:
            node_id: The unique name of the node to find.

        Returns:
            The Agent or Supervisor object if found, otherwise None.
        """
        if not self.root_node:
            return None

        # Use a queue for a breadth-first search through the graph
        q = [self.root_node]
        visited = {self.root_node.name}

        while q:
            current_node = q.pop(0)
            if current_node.name == node_id:
                return current_node  # Node found

            if isinstance(current_node, Supervisor):
                for child in current_node.registered_agents:
                    if child.name not in visited:
                        visited.add(child.name)
                        q.append(child)

        return None  # Node not found in the entire graph

    def is_default_workflow(self) -> bool:
        return self.is_default



================================================
FILE: studio/ui/app.js
================================================
document.addEventListener("DOMContentLoaded", () => {
    const studioContainer = document.querySelector(".studio-container");
    const editWorkflowBtn = document.getElementById("edit-workflow-btn");
    const startChatBtn = document.getElementById("start-chat-btn");
    const resizer = document.querySelector(".resizer");
    const workspacePanel = document.querySelector(".workspace-panel");
    const contextualPanel = document.querySelector(".contextual-panel");
    const log = document.getElementById("log");
    const form = document.getElementById("form");
    const input = document.getElementById("input");
    const configurationView = document.querySelector('.configuration-view');

    const originalPlaceholder = configurationView.innerHTML; // Store the initial placeholder
    
    addLogEntry("event-SYSTEM", "Welcome to XronAI Studio. Please switch to Chat Mode to connect.");
    
    // =========================================================================
    // 1. CANVAS/DRAWFLOW SETUP
    // =========================================================================
    console.log("[DEBUG] 1. Script start. Preparing to create Drawflow editor.");
    const canvasElement = document.getElementById('drawflow');
    const editor = new Drawflow(canvasElement);
    console.log("[DEBUG] 2. Drawflow object created:", editor);
    
    editor.reroute = true;
    editor.reroute_fix_curvature = true;
    editor.force_first_input = false;
    
    editor.start();
    console.log("[DEBUG] 3. editor.start() has been called.");

    // =========================================================================
    // 2. WORKFLOW LOADING AND RENDERING
    // =========================================================================
    async function loadAndRenderWorkflow() {
        console.log("[DEBUG] 5. loadAndRenderWorkflow() function has started.");
        try {
            const response = await fetch('/api/v1/workflow');
            if (!response.ok) {
                throw new Error(`Failed to fetch workflow: ${response.statusText}`);
            }
            const graph = await response.json();
            console.log("[DEBUG] 6. Received workflow data from backend:", graph);

            editor.clear();

            if (graph.nodes && graph.nodes.length > 0) {
                console.log("[DEBUG] 7. Starting to add nodes to canvas...");
                graph.nodes.forEach(node => {
                    const icon = node.data.title.charAt(0);
                    const content = `<div class="node-content"><div class="node-icon">${icon}</div><div class="node-text"><div class="node-title">${node.data.title}</div><div class="node-subtitle">${node.data.subtitle}</div></div></div>`;
                    let inputs = 0, outputs = 0;
                    switch (node.type) {
                        case 'supervisor': inputs = 1; outputs = 1; break;
                        case 'agent': inputs = 1; outputs = 1; break;
                        case 'user': inputs = 0; outputs = 1; break;
                        case 'tool': case 'mcp': inputs = 1; outputs = 0; break;
                    }
                    editor.addNode(node.id, inputs, outputs, node.pos_x, node.pos_y, node.type, {}, content);
                });
                console.log("[DEBUG] 8. Finished adding nodes.");

                graph.edges.forEach(edge => {
                    const sourceNode = editor.getNodeFromId(editor.getNodesFromName(edge.source)[0]);
                    const targetNode = editor.getNodeFromId(editor.getNodesFromName(edge.target)[0]);
                    if (sourceNode && targetNode) {
                        editor.addConnection(sourceNode.id, targetNode.id, 'output_1', 'input_1');
                    }
                });
                console.log("[DEBUG] 9. Finished adding connections.");
            } else {
                 console.log("[DEBUG] 7. No nodes to render.");
            }
        } catch (error) {
            console.error("Error loading workflow:", error);
            addLogEntry("event-ERROR", `Could not load workflow graph: ${error.message}`);
        }
    }

    // =========================================================================
    // 2.5. NEW: NODE INSPECTOR LOGIC
    // =========================================================================
    async function displayNodeDetails(nodeName) {
        try {
            const response = await fetch(`/api/v1/nodes/${nodeName}`);
            if (!response.ok) {
                throw new Error(`Node not found: ${response.statusText}`);
            }
            const details = await response.json();

            let toolsHtml = '<li>None</li>';
            if (details.tools && details.tools.length > 0) {
                toolsHtml = details.tools.map(tool => `<li>${tool}</li>`).join('');
            }

            const detailsHtml = `
                <div class="config-content">
                    <div class="config-header">
                        <span class="node-type-badge ${details.type}">${details.type}</span>
                        <h2>${details.name}</h2>
                    </div>
                    <div class="config-section">
                        <h3>System Message</h3>
                        <p>${details.system_message || '<em>Not set</em>'}</p>
                    </div>
                    <div class="config-section">
                        <h3>Tools</h3>
                        <ul>${toolsHtml}</ul>
                    </div>
                </div>
            `;
            configurationView.innerHTML = detailsHtml;

        } catch (error) {
            console.error("Failed to fetch node details:", error);
            configurationView.innerHTML = `<div class="panel-placeholder"><p>Could not load details for this node.</p></div>`;
        }
    }

    function resetConfigPanel() {
        configurationView.innerHTML = originalPlaceholder;
    }

    editor.on('nodeSelected', (id) => {
        const node = editor.getNodeFromId(id);
        if (node && node.name) {
            displayNodeDetails(node.name);
        }
    });

    editor.on('nodeUnselected', () => {
        resetConfigPanel();
    });

    // =========================================================================
    // 3. MODE SWITCHING LOGIC
    // =========================================================================
    function setMode(mode) {
        if (mode === 'chat') {
            studioContainer.classList.add('chat-mode');
            startChatBtn.classList.add('active');
            editWorkflowBtn.classList.remove('active');
            setCanvasLock(true);
            connectWebSocket();
        } else { // 'design' mode
            studioContainer.classList.remove('chat-mode');
            editWorkflowBtn.classList.add('active');
            startChatBtn.classList.remove('active');
            setCanvasLock(false);
            disconnectWebSocket();
        }
    }
    editWorkflowBtn.addEventListener('click', () => setMode('design'));
    startChatBtn.addEventListener('click', () => setMode('chat'));
    
    function setCanvasLock(isLocked) {
        editor.editor_mode = isLocked ? 'fixed' : 'edit';
    }

    // =========================================================================
    // 4. PANEL RESIZING LOGIC
    // =========================================================================
    let isResizing = false;
    resizer.addEventListener('mousedown', (e) => {
        isResizing = true;
        document.body.style.userSelect = 'none';
        document.body.style.cursor = 'col-resize';
        const startX = e.clientX;
        const startWorkspaceWidth = workspacePanel.offsetWidth;
        function handleMouseMove(e) {
            if (!isResizing) return;
            const deltaX = e.clientX - startX;
            const newWorkspaceWidth = startWorkspaceWidth + deltaX;
            const totalWidth = studioContainer.offsetWidth;
            const newWorkspacePercentage = (newWorkspaceWidth / totalWidth) * 100;
            workspacePanel.style.width = `${newWorkspacePercentage}%`;
            contextualPanel.style.width = `${100 - newWorkspacePercentage}%`;
        }
        function handleMouseUp() {
            isResizing = false;
            document.body.style.userSelect = '';
            document.body.style.cursor = '';
            document.removeEventListener('mousemove', handleMouseMove);
            document.removeEventListener('mouseup', handleMouseUp);
        }
        document.addEventListener('mousemove', handleMouseMove);
        document.addEventListener('mouseup', handleMouseUp);
    });

    // =========================================================================
    // 5. WEBSOCKET CHAT LOGIC
    // =========================================================================
    let ws = null;
    function connectWebSocket() {
        if (ws && ws.readyState === WebSocket.OPEN) return;
        ws = new WebSocket(`ws://${window.location.host}/ws`);
        ws.onopen = () => addLogEntry("event-SYSTEM", "Connection established. Ready to chat.");
        ws.onclose = () => {
            if (studioContainer.classList.contains('chat-mode')) {
                 addLogEntry("event-ERROR", "Connection closed. Please refresh or try re-entering Chat Mode.");
            }
            ws = null;
        };
        ws.onerror = () => addLogEntry("event-ERROR", `WebSocket Error: Could not connect to the server.`);
        ws.onmessage = (event) => {
            const data = JSON.parse(event.data);
            let content = '';
            switch (data.type) {
                case "WORKFLOW_START": content = `<strong>USER QUERY</strong><p>${data.data.user_query}</p>`; addLogEntry("event-USER", content); break;
                case "SUPERVISOR_DELEGATE": content = `<strong>DELEGATING</strong> [${data.data.source.name} → ${data.data.target.name}]<p><strong>Reasoning:</strong> ${data.data.reasoning}</p><p><strong>Query:</strong> ${data.data.query_for_agent}</p>`; addLogEntry("event-SUPERVISOR_DELEGATE", content); break;
                case "AGENT_TOOL_CALL": content = `<strong>TOOL CALL</strong> [${data.data.source.name}]<p><strong>Tool:</strong> ${data.data.tool_name}</p><p><strong>Arguments:</strong><pre>${JSON.stringify(data.data.arguments, null, 2)}</pre></p>`; addLogEntry("event-AGENT_TOOL_CALL", content); break;
                case "AGENT_TOOL_RESPONSE": content = `<strong>TOOL RESPONSE</strong> [${data.data.source.name}]<p><strong>Result:</strong><pre>${data.data.result}</pre></p>`; addLogEntry("event-AGENT_TOOL_RESPONSE", content); break;
                case "AGENT_RESPONSE": content = `<strong>AGENT RESPONSE</strong> [${data.data.source.name}]<p>${data.data.content}</p>`; addLogEntry("event-AGENT_RESPONSE", content); break;
                case "FINAL_RESPONSE": content = `<strong>FINAL RESPONSE</strong> [${data.data.source.name}]<p>${data.data.content}</p>`; addLogEntry("event-FINAL_RESPONSE", content); break;
                case "ERROR": content = `<strong>ERROR</strong> [${data.data.source.name}]<p>${data.data.error_message}</p>`; addLogEntry("event-ERROR", content); break;
                default: content = `<strong>${data.type}</strong><pre>${JSON.stringify(data.data, null, 2)}</pre>`; addLogEntry("event-SYSTEM", content);
            }
        };
    }
    function disconnectWebSocket() {
        if (ws) {
            ws.close();
            addLogEntry("event-SYSTEM", "Connection closed. Entering Design Mode.");
        }
    }
    function addLogEntry(className, content) {
        const entry = document.createElement("div");
        entry.className = `log-entry ${className}`;
        entry.innerHTML = content;
        log.appendChild(entry);
        log.scrollTop = log.scrollHeight;
    }
    form.addEventListener("submit", (event) => {
        event.preventDefault();
        const message = input.value;
        if (message && ws && ws.readyState === WebSocket.OPEN) {
            ws.send(message);
            input.value = '';
        } else if (!ws || ws.readyState !== WebSocket.OPEN) {
            addLogEntry("event-ERROR", "Cannot send message. Not connected.");
        }
    });

    // =========================================================================
    // 6. TOOLBAR & NODE CREATION LOGIC
    // =========================================================================
    const addSupervisorBtn = document.getElementById('add-supervisor-btn');
    const addAgentBtn = document.getElementById('add-agent-btn');
    const addUserBtn = document.getElementById('add-user-btn');
    const addToolBtn = document.getElementById('add-tool-btn');
    const addMcpBtn = document.getElementById('add-mcp-btn');
    
    let nodeCounter = 0;

    addSupervisorBtn.addEventListener('click', () => addNode('supervisor', 'Supervisor', 'Manages agents'));
    addAgentBtn.addEventListener('click', () => addNode('agent', 'Agent', 'Performs tasks'));
    addUserBtn.addEventListener('click', () => addNode('user', 'User', 'Workflow entry point'));
    addToolBtn.addEventListener('click', () => addNode('tool', 'Tool', 'Executes a function'));
    addMcpBtn.addEventListener('click', () => addNode('mcp', 'MCP Server', 'Connects to remote tools'));

    function addNode(type, title, subtitle) {
        if (editor.editor_mode === 'fixed') return;

        nodeCounter++;
        const nodeName = `${title}_${nodeCounter}`; 
        const icon = title.charAt(0);

        const content = `
            <div class="node-content">
                <div class="node-icon">${icon}</div>
                <div class="node-text">
                    <div class="node-title">${title}</div>
                    <div class="node-subtitle">${subtitle}</div>
                </div>
            </div>
        `;
        
        let inputs = 0, outputs = 0;
        switch (type) {
            case 'supervisor': inputs = 1; outputs = 1; break;
            case 'agent': inputs = 1; outputs = 1; break;
            case 'user': inputs = 0; outputs = 1; break;
            case 'tool': case 'mcp': inputs = 1; outputs = 0; break;
        }

        editor.addNode(nodeName, inputs, outputs, 150, 50, type, {}, content);
    }
    
    // =========================================================================
    // 7. INITIALIZATION
    // =========================================================================
    setMode('design');
    
    console.log("[DEBUG] 4. Calling setTimeout to queue workflow loading.");
    setTimeout(() => {
        loadAndRenderWorkflow();
    }, 0);
});


================================================
FILE: studio/ui/index.html
================================================
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/jerosoler/Drawflow/dist/drawflow.min.css">
    <script src="https://cdn.jsdelivr.net/gh/jerosoler/Drawflow/dist/drawflow.min.js"></script>

    <title>XronAI Studio</title>
    <link rel="stylesheet" href="/style.css">
</head>
<body>
    <div class="studio-container">
        <header class="studio-header">
            <h1>XronAI Studio</h1>
            <div class="mode-switcher">
                <button id="edit-workflow-btn" class="active">Design Mode</button>
                <button id="start-chat-btn">Chat Mode</button>
            </div>
        </header>

        <main class="studio-main-content">
            <div class="workspace-panel">
                <aside class="icon-toolbar">
                    <button id="add-supervisor-btn" title="Add Supervisor">S</button>
                    <button id="add-agent-btn" title="Add Agent">A</button>
                    <button id="add-user-btn" title="Add User">U</button>
                    <button id="add-tool-btn" title="Add Tool">T</button>
                    <button id="add-mcp-btn" title="Add MCP Server">M</button>
                </aside>
                <section class="canvas" id="drawflow"></section>
            </div>

            <div class="resizer" data-direction="vertical"></div>

            <section class="contextual-panel">
                <div class="configuration-view">
                     <div class="panel-placeholder">
                        <h2>Configuration</h2>
                        <p>Click a node on the canvas to configure its properties here.</p>
                    </div>
                </div>
                <div class="chat-view">
                    <div id="log" class="log-container"></div>
                    <footer class="chat-footer">
                        <form id="form" class="chat-form">
                            <input type="text" id="input" placeholder="Type your message..." autocomplete="off">
                            <button type="submit">Send</button>
                        </form>
                    </footer>
                </div>
            </section>
        </main>
    </div>
    <script src="/app.js"></script>
</body>
</html>


================================================
FILE: studio/ui/style.css
================================================
/* --- 1. Root Variables & Color Theme --- */
:root {
    --background-dark: #000000;
    --background-light: #111111;
    --surface-color: #1a1a1a;
    --border-color: #333333;
    --font-primary: #f2f2f2;
    --font-secondary: #888888;
    --accent-color: #ffffff;
    --accent-hover: #cccccc;
    --font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;

    /* Colors for log entries */
    --color-user: #3b82f6; --color-delegate: #f59e0b; --color-tool-call: #8b5cf6;
    --color-tool-response: #06b6d4; --color-agent-response: #d946ef; --color-final: #22c55e;
    --color-error: #ef4444; --color-system: #888888;

    /* UPDATED: Added colors for new nodes */
    --color-node-supervisor: #f59e0b; /* Amber */
    --color-node-agent: #8b5cf6;      /* Violet */
    --color-node-user: #3b82f6;       /* Blue */
    --color-node-tool: #22c55e;       /* Green */
    --color-node-mcp: #ef4444;        /* Red */

    /* UPDATED: Specific colors for ports */
    --color-port-input: #f59e0b;  /* Amber/Yellow for receiving tasks */
    --color-port-output: #f2f2f2; /* White for delegating tasks */
}

/* --- 2. Global Styles --- */
*, *::before, *::after { box-sizing: border-box; }
body, html {
    margin: 0; padding: 0; height: 100vh; width: 100vw;
    font-family: var(--font-family); background-color: var(--background-dark);
    color: var(--font-primary); overflow: hidden;
}

/* --- 3-8. (Layout, Mode Switcher, Panels, Chat UI - NO CHANGES) --- */
.studio-container { display: flex; flex-direction: column; height: 100%; width: 100%; }
.studio-header { display: flex; justify-content: space-between; align-items: center; padding: 0 24px; height: 60px; border-bottom: 1px solid var(--border-color); flex-shrink: 0; }
.studio-header h1 { font-size: 1.25rem; font-weight: 600; }
.mode-switcher button { padding: 8px 16px; font-size: 14px; background-color: transparent; color: var(--font-secondary); border: 1px solid var(--border-color); cursor: pointer; transition: background-color 0.2s, color 0.2s; }
.mode-switcher button:first-child { border-top-left-radius: 6px; border-bottom-left-radius: 6px; border-right: none; }
.mode-switcher button:last-child { border-top-right-radius: 6px; border-bottom-right-radius: 6px; }
.mode-switcher button.active { background-color: var(--accent-color); color: var(--background-dark); border-color: var(--accent-color); }
.studio-main-content { display: flex; flex-grow: 1; height: calc(100vh - 60px); }
.workspace-panel { width: 60%; display: flex; height: 100%; min-width: 400px; flex-grow: 1; }
.icon-toolbar { display: flex; flex-direction: column; align-items: center; padding: 12px 0; width: 60px; background-color: var(--background-dark); border-right: 1px solid var(--border-color); flex-shrink: 0; }
.icon-toolbar button { width: 40px; height: 40px; margin-bottom: 12px; font-size: 20px; font-weight: bold; background-color: var(--surface-color); border: 1px solid var(--border-color); color: var(--font-secondary); border-radius: 8px; cursor: pointer; }
.canvas { flex-grow: 1; background-color: var(--background-light); background-image: radial-gradient(var(--border-color) 1px, transparent 1px); background-size: 16px 16px; position: relative; }
.contextual-panel { width: 40%; min-width: 400px; height: 100%; background-color: var(--background-dark); border-left: 1px solid var(--border-color); display: flex; flex-direction: column; flex-shrink: 1; }
.panel-placeholder { padding: 2rem; color: var(--font-secondary); text-align: center; }
.configuration-view { display: flex; align-items: center; justify-content: center; height: 100%; }
.chat-view { display: none; }
.studio-container.chat-mode .configuration-view { display: none; }
.studio-container.chat-mode .chat-view { display: flex; flex-direction: column; height: 100%; }
.studio-container.chat-mode .workspace-panel { pointer-events: none; opacity: 0.6; }
.resizer { width: 5px; background-color: transparent; cursor: col-resize; flex-shrink: 0; position: relative; transition: background-color 0.2s; }
.resizer:hover { background-color: var(--accent-color); }
.log-container { flex-grow: 1; overflow-y: auto; padding: 1rem; }
.log-entry { padding: 0.75rem 1rem; margin-bottom: 0.75rem; border-radius: 8px; border: 1px solid var(--border-color); background-color: var(--background-light); white-space: pre-wrap; word-wrap: break-word; font-size: 14px; }
.log-entry strong { display: block; margin-bottom: 0.5rem; font-size: 0.8em; font-weight: 600; text-transform: uppercase; letter-spacing: 0.5px; }
.event-USER strong { color: var(--color-user); } .event-SUPERVISOR_DELEGATE strong { color: var(--color-delegate); } .event-AGENT_TOOL_CALL strong { color: var(--color-tool-call); }
.event-AGENT_TOOL_RESPONSE strong { color: var(--color-tool-response); } .event-AGENT_RESPONSE strong { color: var(--color-agent-response); } .event-FINAL_RESPONSE strong { color: var(--color-final); }
.event-ERROR strong { color: var(--color-error); } .event-SYSTEM strong { color: var(--color-system); }
.event-SYSTEM { color: var(--font-secondary); font-style: italic; }
.chat-footer { padding: 1rem; border-top: 1px solid var(--border-color); flex-shrink: 0; }
.chat-form { display: flex; }
#input { flex-grow: 1; padding: 0.75rem 1rem; border: 1px solid var(--border-color); border-radius: 8px; background-color: var(--background-light); color: var(--font-primary); font-size: 1rem; }
#input:focus { outline: none; border-color: var(--accent-color); }
.chat-form button { padding: 0.75rem 1.5rem; margin-left: 0.75rem; border: none; border-radius: 8px; background-color: var(--accent-color); color: var(--background-dark); font-size: 1rem; font-weight: 600; cursor: pointer; }

/* ========================================================================= */
/* --- 9. UPDATED Drawflow Node & Connection Styles --- */
/* ========================================================================= */
.drawflow-node { background: var(--surface-color); border: 1px solid var(--border-color); border-radius: 8px; box-shadow: 0 4px 12px rgba(0,0,0,0.5); width: 250px !important; font-family: var(--font-family); color: var(--font-primary); padding: 0 !important; }
.drawflow-node.selected { border-color: var(--accent-color); box-shadow: 0 0 0 1px var(--accent-color); }
.drawflow-node .drawflow_content_node .node-content { display: flex; align-items: center; padding: 12px; }
.node-icon { width: 40px; height: 40px; flex-shrink: 0; border-radius: 6px; margin-right: 12px; display: flex; align-items: center; justify-content: center; font-size: 20px; font-weight: bold; }
.node-text { flex-grow: 1; }
.node-title { font-size: 14px; font-weight: 600; color: var(--font-primary); margin-bottom: 2px; }
.node-subtitle { font-size: 12px; color: var(--font-secondary); }

/* Specific styling for ALL node types */
.drawflow-node.supervisor .node-icon { background-color: var(--color-node-supervisor); color: var(--background-dark); }
.drawflow-node.agent .node-icon { background-color: var(--color-node-agent); color: var(--background-dark); }
.drawflow-node.user .node-icon { background-color: var(--color-node-user); color: var(--background-dark); }
.drawflow-node.tool .node-icon { background-color: var(--color-node-tool); color: var(--background-dark); }
.drawflow-node.mcp .node-icon { background-color: var(--color-node-mcp); color: var(--background-dark); }

/* UPDATED: Specific styling for Input/Output Ports */
.drawflow-node .input, .drawflow-node .output { border: 2px solid var(--surface-color); width: 16px; height: 16px; border-radius: 50%; position: absolute; top: 50%; transform: translateY(-50%); }
.drawflow-node .input { background: var(--color-port-input); left: -9px; }
.drawflow-node .output { background: var(--color-port-output); right: -9px; }

/* Connection Lines */
.drawflow .drawflow-connection .main-path { stroke-width: 2px; stroke: var(--font-secondary); filter: drop-shadow(0 1px 1px rgba(0,0,0,0.3)); }

/* --- 10. NEW: Configuration Panel Styles --- */
.config-content {
    width: 100%;
    height: 100%;
    padding: 1.5rem;
    text-align: left;
    overflow-y: auto;
}
.config-header {
    display: flex;
    align-items: center;
    margin-bottom: 1.5rem;
    padding-bottom: 1rem;
    border-bottom: 1px solid var(--border-color);
}
.config-header h2 {
    margin: 0 0 0 1rem;
    font-size: 1.25rem;
}
.node-type-badge {
    padding: 4px 10px;
    border-radius: 12px;
    font-size: 0.8rem;
    font-weight: 600;
    text-transform: uppercase;
    color: var(--background-dark);
}
.node-type-badge.supervisor { background-color: var(--color-node-supervisor); }
.node-type-badge.agent { background-color: var(--color-node-agent); }
.node-type-badge.user { background-color: var(--color-node-user); }

.config-section {
    margin-bottom: 1.5rem;
}
.config-section h3 {
    font-size: 0.9rem;
    font-weight: 600;
    color: var(--font-secondary);
    text-transform: uppercase;
    letter-spacing: 0.5px;
    margin-bottom: 0.75rem;
}
.config-section p {
    margin: 0;
    line-height: 1.6;
    color: var(--font-primary);
    white-space: pre-wrap;
}
.config-section ul {
    list-style: none;
    padding: 0;
    margin: 0;
}
.config-section li {
    background-color: var(--surface-color);
    padding: 0.5rem 0.75rem;
    border-radius: 4px;
    margin-bottom: 0.5rem;
    font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;
    font-size: 0.9rem;
}


================================================
FILE: .github/workflows/publish-to-pypi.yml
================================================
name: Publish Python Package to PyPI

# This is the trigger: the workflow runs ONLY when a new release is created on GitHub.
on:
  release:
    types: [published]

jobs:
  # The 'build-and-publish' job defines all the steps needed.
  build-and-publish:
    name: Build and publish Python package to PyPI
    runs-on: ubuntu-latest # The job runs on a fresh virtual machine hosted by GitHub.

    steps:
      # Step 1: Check out your repository's code so the job can access it.
      - name: Checkout repository
        uses: actions/checkout@v4

      # Step 2: Set up the Python environment.
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.9" # You can change this to your preferred version.

      # Step 3: Install the necessary build tools.
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install build

      # Step 4: Build the package (this creates the .whl and .tar.gz files).
      - name: Build package
        run: python -m build

      # Step 5: This is the key publishing step.
      - name: Publish package to PyPI
        # This uses a pre-made, trusted action from the Python Packaging Authority (PyPA).
        uses: pypa/gh-action-pypi-publish@release/v1
        with:
          # The password is automatically and securely pulled from your GitHub Secrets.
          password: ${{ secrets.PYPI_API_TOKEN }}


---


I have provided you the complete code of the SDK I am working on. I would like you to take a deep look in the code and try to understand what is happening. This is important for you to understand the complete code because later I will be asking for your help to continue developing the SDK. 